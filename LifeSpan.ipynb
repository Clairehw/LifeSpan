{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Insight1\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math, re, time, random\n",
    "from scipy import stats\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "os.chdir(\"I:\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# os.chdir(\"C:\\\\Users\\\\Insight1\\\\Anaconda3\")\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Preprocess and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average remaining life span: 96.808266996657 \n",
      "Mode of remaining life span: [72]\n"
     ]
    }
   ],
   "source": [
    "# read data from pickle file\n",
    "data = pd.read_pickle('notes_with_age_and_impairment.pkl')\n",
    "\n",
    "# change data types to float and int for age and life expectancy\n",
    "data['age'] = data['age'].astype(float)\n",
    "data['le'] = data['le'].astype(int)\n",
    "data['le_year'] = data['le'].apply(lambda x: round(x/12))\n",
    "\n",
    "data['notes_split'] = data['notes'].str.replace('\\n', ' ').str.replace(r' +', ' ').str.split(' ')\n",
    "data['len_notes'] = data['notes_split'].apply(len)\n",
    "\n",
    "ave_le = np.average(data['le'].to_numpy())\n",
    "mode_le = stats.mode(data['le'].to_numpy())\n",
    "print('Average remaining life span: {} \\nMode of remaining life span: {}'.format(ave_le, mode_le[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 First glance of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1512x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First glanse of our data\n",
    "eda = ['age','le','len_notes']\n",
    "title = ['Age Distribution','Remaining Life Span','Medical Notes Length']\n",
    "color = ['gold','aqua','pink']\n",
    "xla =['Age (year)','Month','Words per Note']\n",
    "# data_eda = data[]\n",
    "fig, ax = plt.subplots(1,3, figsize=(21,4))\n",
    "for i, (a, eda) in enumerate(zip(ax,eda)):\n",
    "    a.hist(data[eda], bins=50, color=color[i], ec='black')\n",
    "    a.set_ylabel('Counts', fontsize=10,fontweight ='bold')\n",
    "    a.set_xlabel(xla[i], fontsize=10,fontweight ='bold')\n",
    "    a.set_title(title[i], fontsize=12, fontweight ='bold',pad =5)\n",
    "\n",
    "# plt.figure(figsize=(4,3))\n",
    "# plt.hist(data['age'], bins=50, color='gold', ec='black')\n",
    "# plt.xlabel('Age (year)', fontsize=10,fontweight ='bold')\n",
    "# plt.ylabel('Counts', fontsize=10,fontweight ='bold')\n",
    "# plt.title('Age Distribution', fontsize=12, fontweight ='bold',pad =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocess (remove missing/wrong/unrealistic samples) & EDA graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54143, 8) (52516, 8)\n"
     ]
    }
   ],
   "source": [
    "filt = (data['len_notes']<=2000) & (data['len_notes']>=10) & (data['le'] <=250) & (data['age'] <=110) & (data['age'] >=60)\n",
    "datafilt = data[filt]\n",
    "print(data.shape, datafilt.shape) #97% data remained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1512x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eda = ['age','le','len_notes']\n",
    "title = ['Age Distribution','Remaining Life Span','Medical Notes Length']\n",
    "color = ['gold','aqua','pink']\n",
    "xla =['Age (year)','Month','Words per Note']\n",
    "# data_eda = data[]\n",
    "fig, ax = plt.subplots(1,3, figsize=(21,4))\n",
    "for i, (a, eda) in enumerate(zip(ax,eda)):\n",
    "    a.hist(datafilt[eda], bins=50, color=color[i], ec='black')\n",
    "    a.set_ylabel('Counts', fontsize=10,fontweight ='bold')\n",
    "    a.set_xlabel(xla[i], fontsize=10,fontweight ='bold')\n",
    "    a.set_title(title[i], fontsize=12, fontweight ='bold',pad =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Insight1\\\\Anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordCloud' from 'wordcloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-3c404ed104a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# data = pd.read_pickle('clean_note_spacy_vec.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfiltwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'len_notes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'len_notes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnotewc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfiltwc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WordCloud' from 'wordcloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "# data = pd.read_pickle('clean_note_spacy_vec.pkl')\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "filtwc = (data['len_notes'] >= 500) & (data['len_notes'] <= 600)\n",
    "notewc = data.loc[filtwc, 'notes']\n",
    "notewc.index = range(notewc.shape[0])\n",
    "\n",
    "a = random.randrange(len(notewc))\n",
    "\n",
    "STOPWORDS.update(['noted', 'was', 'say','Was','reported','cited','Vv', 'vv','Wa'])\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords)\n",
    "wc.generate(notewc[a])\n",
    "\n",
    "plt.figure(figsize = (6, 6), facecolor = None) \n",
    "plt.imshow(wc) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() \n",
    "# wc.to_file('note_wc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning and Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Remove non alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from scipy import stats\n",
    "def clean(note):\n",
    "    table = str.maketrans('', '','!\"#$%&\\'()*+,-./:;<=>?@‘¥£’—“[\\\\]^_`{|}~«»§é')\n",
    "    s = note.translate(table)\n",
    "    s1 = s.replace('\\n',' ').lower()\n",
    "    s2 = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', s1)\n",
    "    st_clean = re.sub(r' +', ' ', s2)\n",
    "    return st_clean\n",
    "\n",
    "data['notes_clean'] = data['notes'].progress_apply(lambda x: clean(x))\n",
    "data['len_clean'] = data['notes_clean'].progress_apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# data['spacy_notes_clean'] = data['notes_clean'].progress_apply(lambda x: nlp(x).vector)\n",
    "# data.to_pickle('clean_note_spacy_vec.pkl')\n",
    "# saved pkl file NO.1\n",
    "# columns =['key', 'notes', 'le', 'age', 'primary_impairment', 'le_year',\n",
    "#           'notes_split','len_notes', 'notes_clean', 'len_clean', 'spacy_notes_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saved pkl file NO.1 columns =\n",
    "**`['key', 'notes', 'le', 'age', 'primary_impairment', 'le_year','notes_split','len_notes',\n",
    "'notes_clean', 'len_clean', 'spacy_notes_clean']`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenization, remove stopwords, stemming, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_stem_stop(note):\n",
    "    word_tokens = word_tokenize(note)\n",
    "    \n",
    "    # lemmatization\n",
    "    lemma_words = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for w in word_tokens:\n",
    "        word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "        word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "        word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "        lemma_words.append(word3)\n",
    "\n",
    "    # STOPWORDS\n",
    "    states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "              \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \n",
    "              \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n",
    "              \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \n",
    "              \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "    states = [x.lower() for x in states]\n",
    "    \n",
    "    stopwd = set(stopwords.words('english'))\n",
    "    stopwd.update(['note', 'was', 'say','cite','report','vv','ii','iii','report', 'disclaim','disclaimer',\n",
    "                   'request','underwrite','underwriter','life','expect','certify','take','company',\n",
    "                  'id','written','write','ssn','social','security','number','llc','use']) \n",
    "    stopwd.update(states)\n",
    "    filtered_sentence = [w for w in lemma_words if not w in stopwd] # can switch lemma or stem words\n",
    "    \n",
    "    #Stemming\n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    \n",
    "    lemma_stop_stem = Stem_words\n",
    "    return lemma_stop_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lss_notes'] = data['notes_clean'].progress_apply(lambda x: lemma_stem_stop(x))\n",
    "data['len_lss'] = data['lss_notes'].progress_apply(lambda x: len(x))\n",
    "data['lss_corpus'] = data['lss_notes'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('clean_note_spacy_vec_lss_spacy.pkl')\n",
    "# saved pkl file NO.2\n",
    "# columns = ['key', 'notes', 'le', 'age', 'primary_impairment', 'le_year', 'notes_split', 'len_notes', 'notes_clean', \n",
    "#           'len_clean', 'spacy_notes_clean', 'lss_notes', 'len_lss', 'lss_corpus','spacy_lss_corpus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saved pkl file NO.2 columns =\n",
    "**`['key', 'notes', 'le', 'age', 'primary_impairment', 'le_year', 'notes_split', 'len_notes', \n",
    "'notes_clean', 'len_clean', 'spacy_notes_clean', \n",
    "'lss_notes', 'len_lss', 'lss_corpus', 'spacy_lss_corpus']`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Extraction (Vectorize by spacy, or Bag of words, or TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize cleaned corpus by spacy \n",
    "data['spacy_lss_corpus'] = data['lss_corpus'].progress_apply(lambda x: nlp(x).vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['key', 'notes', 'le', 'age', 'primary_impairment', 'le_year',\n",
       "       'notes_split', 'len_notes', 'notes_clean', 'len_clean',\n",
       "       'spacy_notes_clean', 'lss_notes', 'len_lss', 'lss_corpus',\n",
       "       'spacy_lss_corpus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('clean_note_spacy_vec_lss_spacy.pkl') #load data for tf-idf\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.chdir(\"C:\\\\Users\\\\Insight1\\\\Anaconda3\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# transform len_lss\n",
    "filt = (data['len_lss']<=2000) & (data['len_lss']>=50) & (data['le'] <=250) & (data['age'] <=100) & (data['age'] >=60)\n",
    "data_tfidf = data[filt][['age', 'le','le_year','lss_notes','lss_corpus']]\n",
    "\n",
    "corpus = data_tfidf['lss_corpus'].tolist()\n",
    "vec = CountVectorizer()\n",
    "bag_of_words = vec.fit_transform(corpus)\n",
    "sum_words = bag_of_words.sum(axis=0) # sum word counts\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# return words_freq[:n]\n",
    "vocab = [i[0] for i in words_freq[:4000]] # define vocabulary for tf-idf data most frequent 4000 words\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "corpus_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load spacy vectorized notes for  Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_pickle('clean_note_spacy_vec.pkl')\n",
    "\n",
    "filt = (data['len_notes']<=2000) & (data['len_notes']>=50) & (data['le'] <=250) & (data['age'] <=100) & (data['age'] >=60)\n",
    "data1 = data[filt]\n",
    "\n",
    "X = data1['spacy_notes_clean'].to_numpy()\n",
    "X = np.array([i for i in X]).reshape(52475,300) #X = np.concatenate(X).reshape(54143,300)\n",
    "x_age = np.expand_dims(data1['age'].to_numpy(), axis=1)\n",
    "\n",
    "X = np.hstack((x_age, X))\n",
    "y = data1['le'].to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Load spacy vectorize lss_corpus for Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle('clean_note_spacy_vec_lss_spacy.pkl')\n",
    "\n",
    "filt = (data['len_notes']<=2000) & (data['len_notes']>=50) & (data['le'] <=250) & (data['age'] <=100) & (data['age'] >=60)\n",
    "data_spacylss_filt = data[filt]\n",
    "\n",
    "X = np.array(data_spacylss_filt['spacy_lss_corpus'].values.tolist())\n",
    "x_age = np.expand_dims(data_spacylss_filt['age'].to_numpy(), axis=1)\n",
    "\n",
    "# X y from spacy vector of lss_corpus for Model\n",
    "X = np.hstack((x_age, X))\n",
    "y = data_spacylss_filt['le'].to_numpy() \n",
    "# y = data_spacylss_filt['le_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save X y numpy to pkl.npy for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus_tfidf.toarray()\n",
    "x_age = np.expand_dims(data_tfidf['age'].to_numpy(), axis = 1)\n",
    "\n",
    "# X y from tf-idf for Model\n",
    "X = np.hstack((x_age, X))\n",
    "y = data_tfidf['le'].to_numpy() \n",
    "# y = data_spacylss_filt['le_year']\n",
    "\n",
    "# save X and y to pickle file\n",
    "# np.save('tfidf_X_4001.pkl', X)\n",
    "# np.save('tfidf_y.pkl', y)\n",
    "# X=np.load('tfidf_X_4001.pkl.npy', allow_pickle=True)\n",
    "# y=np.load('tfidf_y.pkl.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 80%/20% split for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 70%/10%/20% split for train/validation/test\n",
    "# X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of age based on min and max of x-train\n",
    "a = X_train[:, 0].max()\n",
    "b = X_train[:, 0].min()\n",
    "X_train[:, 0] = (X_train[:, 0]-b)/(a-b)\n",
    "X_test[:, 0] = (X_test[:, 0]-b)/(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y_pred dataframe to store y_pred from model\n",
    "# df_score dataframe to store mae, train_r2, test_r2\n",
    "df_pred_ytest = pd.DataFrame()\n",
    "df_pred_ytrain = pd.DataFrame()\n",
    "df_score = pd.DataFrame(index=['MAE','train_r2','test_r2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 RF model & feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor score = 0.5402527642512915 for 57k data\n",
    "# rf regressor r2 = 0.5560166555170266 when filted len_word<=750, le_months<=250\n",
    "# rf regressor r2 = 0.5384426783146021 when filted 100<=len_word<=2000, le_months<=250 on noteclean_spacy_vec\n",
    "# rf reg spacy_lss r2= 0.5800088178042535 for 50-1000 len_lss, le_months<250\n",
    "# r2 = 0.9483540267963761 0.6502525420270096 for n_estimator 100\n",
    "# r2 = 0.9482949319146955 0.6514315694118593 for n_estimator 50\n",
    "# r2 = 0.9482869107367904 0.6494406487369414 for n_estimator 30\n",
    "# the above are r2 for notes without age\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "reg_rf = RandomForestRegressor(n_estimators = 30, random_state = 0)\n",
    "\n",
    "reg_rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "y_test_rf = reg_rf.predict(X_test)\n",
    "y_train_rf = reg_rf.predict(X_train)\n",
    "\n",
    "# feature importances from model\n",
    "importances = reg_rf.feature_importances_\n",
    "\n",
    "# R squared from model\n",
    "rf_train_score = reg_rf.score(X_train, y_train)\n",
    "rf_test_score = reg_rf.score(X_test, y_test)\n",
    "\n",
    "# calculate mean absolute residual\n",
    "rf_mae = sum(np.absolute(y_test_rf - y_test))/y_test.size\n",
    "\n",
    "\n",
    "print(rf_mae, rf_train_score, rf_test_score)\n",
    "# r2 = 0.983937158661077 0.8860908022722117\n",
    "# mae = 9.713297711394068   r2 =0.9841633938912728 0.8873791244281546 for n_est =30\n",
    "# mae = 9.788619451742454   r2 =0.9840451957775558 0.8854066362163049 for n_est =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted training and testing data into dataframe\n",
    "df_pred_ytrain['y_train_rf'] = y_train_rf\n",
    "df_pred_ytest['y_test_rf'] = y_test_rf\n",
    "df_score['rf'] =[rf_mae, rf_train_score, rf_test_score]\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "vip_idx = np.argsort(np.absolute(importances))[::-1]\n",
    "importances_sort = [importances[i] for i in vip_idx]\n",
    "\n",
    "vocab.insert(0,'age') \n",
    "vip_name = [vocab[i] for i in vip_idx]\n",
    "print(len(vocab), len(vip_name))\n",
    "\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.barh(range(30)[::-1], importances_sort[1:31], color = 'peru')\n",
    "plt.yticks(range(30)[::-1], vip_name[1:31], rotation =0, fontsize =12)\n",
    "plt.ylim(-0.5,29.7)\n",
    "plt.title('Feature Importance', fontweight='bold', pad =10, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot R-squared graphs\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_train[1000:1500], y_train_rf[1000:1500], label = 'Train', color='black')\n",
    "plt.scatter(y_test[1000:1500], y_test_rf[1000:1500], label = 'Test', color ='orange')\n",
    "plt.plot(range(300), range(300), '-k',linestyle ='--')\n",
    "plt.xlim(0, 260)\n",
    "plt.ylim(0, 260)\n",
    "plt.title('Random Forest Regression', fontsize =26, fontweight = 'bold', pad =10)\n",
    "plt.xlabel('Observed',fontsize =24, fontweight = 'bold')\n",
    "plt.ylabel('Predicted',fontsize =24, fontweight = 'bold')\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "a = plt.legend(loc='upper left', fontsize = 16)\n",
    "a.legendHandles[0]._sizes =[90]\n",
    "a.legendHandles[1]._sizes =[90]\n",
    "plt.show()\n",
    "# plt.savefig('Random Forest predicted vs observed.png', dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual graphs\n",
    "\n",
    "# residual = np.absolute(y_test_rf - y_test)\n",
    "residual = y_test_rf - y_test\n",
    "\n",
    "plt.figure(figsize= (4,4))\n",
    "plt.scatter(y_test[:1500], residual[:1500])\n",
    "plt.axhline(y=rf_mae, color ='k', linestyle ='--')\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(-20, 40)\n",
    "plt.title('Random Forest Residual', fontsize =16, fontweight = 'bold', pad =10)\n",
    "plt.xlabel('Observed (month)',fontsize =12, fontweight = 'bold')\n",
    "plt.ylabel('Residual (month)',fontsize =12, fontweight = 'bold')\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "# plt.show()\n",
    "# plt.savefig('RF Residual.png', dpi =400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RF model with only age as input (ablation study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age as the only input\n",
    "reg_rf = RandomForestRegressor(n_estimators = 40, random_state = 0)\n",
    "reg_rf = RandomForestRegressor().fit(np.expand_dims(X_train[:, 0], axis=1), y_train)\n",
    "\n",
    "y_test_rf1 = reg_rf.predict(np.expand_dims(X_test[:, 0], axis=1))\n",
    "y_train_rf1 = reg_rf.predict(np.expand_dims(X_train[:, 0], axis=1))\n",
    "\n",
    "rf_train_score1 = reg_rf.score(np.expand_dims(X_train[:, 0], axis=1), y_train)\n",
    "rf_test_score1 = reg_rf.score(np.expand_dims(X_test[:, 0], axis=1), y_test)\n",
    "\n",
    "rf1_mae = sum(np.absolute(y_test_rf1-y_test))/y_test.size\n",
    "\n",
    "print(rf1_mae, rf_train_score1, rf_test_score1) # R^2 variance weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted training and testing data into dataframe\n",
    "df_pred_ytrain['y_train_rf_age'] = y_train_rf1\n",
    "df_pred_ytest['y_test_rf_age'] = y_test_rf1\n",
    "df_score['rf_age'] =[rf1_mae, rf_train_score1, rf_test_score1]\n",
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Save df_pred_ytrain, df_pred_ytest, df_score to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save predicted y values and R2 scores to csv file\n",
    "df_pred_ytrain.to_csv('df_pred_ytrain.csv')\n",
    "df_pred_ytest.to_csv('df_pred_ytest.csv')\n",
    "df_score.to_csv('df_score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Multilayer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               1024512   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,059,073\n",
      "Trainable params: 1,058,305\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Train on 37114 samples, validate on 5302 samples\n",
      "Epoch 1/1000\n",
      "37114/37114 [==============================] - 2s 65us/sample - loss: 11162.0977 - val_loss: 11040.6162\n",
      "Epoch 2/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10898.7809 - val_loss: 11055.6504\n",
      "Epoch 3/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10679.7857 - val_loss: 11075.4746\n",
      "Epoch 4/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10518.5441 - val_loss: 11100.1738\n",
      "Epoch 5/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10401.3106 - val_loss: 11124.8516\n",
      "Epoch 6/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10310.1492 - val_loss: 11146.1162\n",
      "Epoch 7/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10227.7471 - val_loss: 11172.1045\n",
      "Epoch 8/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 10147.8106 - val_loss: 11209.4727\n",
      "Epoch 9/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10075.3597 - val_loss: 11258.0576\n",
      "Epoch 10/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 10007.0796 - val_loss: 11310.4219\n",
      "Epoch 11/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9934.5960 - val_loss: 11363.1836\n",
      "Epoch 12/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 9864.5182 - val_loss: 11410.7988\n",
      "Epoch 13/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9796.2740 - val_loss: 11454.1807\n",
      "Epoch 14/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9728.1490 - val_loss: 11499.1426\n",
      "Epoch 15/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 9659.2390 - val_loss: 11544.5723\n",
      "Epoch 16/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9592.4698 - val_loss: 11585.4414\n",
      "Epoch 17/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9525.9862 - val_loss: 11629.8389\n",
      "Epoch 18/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9462.1392 - val_loss: 11678.0742\n",
      "Epoch 19/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9389.8770 - val_loss: 11717.0381\n",
      "Epoch 20/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9324.0622 - val_loss: 11761.5889\n",
      "Epoch 21/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 9255.9611 - val_loss: 11800.3086\n",
      "Epoch 22/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9190.2988 - val_loss: 11835.4551\n",
      "Epoch 23/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9115.1668 - val_loss: 11869.4990\n",
      "Epoch 24/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 9056.0608 - val_loss: 11887.3545\n",
      "Epoch 25/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8989.9658 - val_loss: 11909.2227\n",
      "Epoch 26/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8916.6040 - val_loss: 11918.1328\n",
      "Epoch 27/1000\n",
      "37114/37114 [==============================] - ETA: 0s - loss: 8844.48 - 1s 15us/sample - loss: 8852.4846 - val_loss: 11921.0713\n",
      "Epoch 28/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8781.8113 - val_loss: 11911.5078\n",
      "Epoch 29/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8712.7977 - val_loss: 11895.7031\n",
      "Epoch 30/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8647.1782 - val_loss: 11839.7129\n",
      "Epoch 31/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 8583.4832 - val_loss: 11828.6221\n",
      "Epoch 32/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8509.2545 - val_loss: 11777.2461\n",
      "Epoch 33/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8448.8565 - val_loss: 11739.3965\n",
      "Epoch 34/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8379.3138 - val_loss: 11709.0488\n",
      "Epoch 35/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8312.6824 - val_loss: 11654.3301\n",
      "Epoch 36/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8248.8026 - val_loss: 11583.0205\n",
      "Epoch 37/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 8187.0843 - val_loss: 11533.6309\n",
      "Epoch 38/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8121.7334 - val_loss: 11491.9756\n",
      "Epoch 39/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 8056.9211 - val_loss: 11434.8809\n",
      "Epoch 40/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 7995.0697 - val_loss: 11370.0947\n",
      "Epoch 41/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 7936.8894 - val_loss: 11306.5303\n",
      "Epoch 42/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 7865.0964 - val_loss: 11219.8418\n",
      "Epoch 43/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 7811.6093 - val_loss: 11118.9199\n",
      "Epoch 44/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7750.6287 - val_loss: 11031.2227\n",
      "Epoch 45/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 7690.2204 - val_loss: 10904.2334\n",
      "Epoch 46/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7630.3171 - val_loss: 10810.9629\n",
      "Epoch 47/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7576.6604 - val_loss: 10719.0342\n",
      "Epoch 48/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 7523.1821 - val_loss: 10620.0908\n",
      "Epoch 49/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7462.6468 - val_loss: 10520.7168\n",
      "Epoch 50/1000\n",
      "37114/37114 [==============================] - 1s 29us/sample - loss: 7402.3357 - val_loss: 10418.6904\n",
      "Epoch 51/1000\n",
      "37114/37114 [==============================] - 1s 29us/sample - loss: 7342.5233 - val_loss: 10292.3135\n",
      "Epoch 52/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 7286.5470 - val_loss: 10166.2676\n",
      "Epoch 53/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7227.9446 - val_loss: 10047.5742\n",
      "Epoch 54/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 7175.0745 - val_loss: 9976.6426\n",
      "Epoch 55/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 7112.4191 - val_loss: 9880.6553\n",
      "Epoch 56/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 7059.3972 - val_loss: 9723.1172\n",
      "Epoch 57/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 7000.5888 - val_loss: 9565.6533\n",
      "Epoch 58/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 6940.9927 - val_loss: 9429.5947\n",
      "Epoch 59/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 6879.1136 - val_loss: 9299.4824\n",
      "Epoch 60/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6819.7187 - val_loss: 9164.8008\n",
      "Epoch 61/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6757.9339 - val_loss: 8987.2441\n",
      "Epoch 62/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6692.8983 - val_loss: 8757.5996\n",
      "Epoch 63/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6627.8483 - val_loss: 8543.6377\n",
      "Epoch 64/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6563.0157 - val_loss: 8406.7773\n",
      "Epoch 65/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6499.1653 - val_loss: 8246.4160\n",
      "Epoch 66/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 6432.7091 - val_loss: 8071.2969\n",
      "Epoch 67/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6361.0830 - val_loss: 7909.0439\n",
      "Epoch 68/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 6294.2625 - val_loss: 7824.5068\n",
      "Epoch 69/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 6217.7016 - val_loss: 7709.5376\n",
      "Epoch 70/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 6155.3585 - val_loss: 7580.2852\n",
      "Epoch 71/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6081.1105 - val_loss: 7416.9839\n",
      "Epoch 72/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 6009.4967 - val_loss: 7279.9185\n",
      "Epoch 73/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 5941.6562 - val_loss: 7170.0498\n",
      "Epoch 74/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 5866.4643 - val_loss: 7039.5762\n",
      "Epoch 75/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 5788.1586 - val_loss: 6901.4199\n",
      "Epoch 76/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 5709.6818 - val_loss: 6803.9531\n",
      "Epoch 77/1000\n",
      "37114/37114 [==============================] - 1s 40us/sample - loss: 5640.9375 - val_loss: 6711.7524\n",
      "Epoch 78/1000\n",
      "37114/37114 [==============================] - 1s 30us/sample - loss: 5563.9507 - val_loss: 6581.6372\n",
      "Epoch 79/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 5492.8308 - val_loss: 6475.2192\n",
      "Epoch 80/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 5414.8302 - val_loss: 6359.6265\n",
      "Epoch 81/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 5337.5860 - val_loss: 6243.8110\n",
      "Epoch 82/1000\n",
      "37114/37114 [==============================] - 1s 35us/sample - loss: 5259.1814 - val_loss: 6126.3433\n",
      "Epoch 83/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 5187.7501 - val_loss: 6010.3276\n",
      "Epoch 84/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 5101.3770 - val_loss: 5905.6045\n",
      "Epoch 85/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 5031.3422 - val_loss: 5778.4653\n",
      "Epoch 86/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4951.6540 - val_loss: 5676.9966\n",
      "Epoch 87/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 4869.4067 - val_loss: 5571.5410\n",
      "Epoch 88/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4783.9227 - val_loss: 5455.0278\n",
      "Epoch 89/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4719.1702 - val_loss: 5315.5562\n",
      "Epoch 90/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 4633.7667 - val_loss: 5200.7891\n",
      "Epoch 91/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 4552.1602 - val_loss: 5086.9043\n",
      "Epoch 92/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 4473.3759 - val_loss: 4998.7515\n",
      "Epoch 93/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4405.4315 - val_loss: 4860.0073\n",
      "Epoch 94/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 4325.6641 - val_loss: 4770.1875\n",
      "Epoch 95/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4238.2990 - val_loss: 4684.9189\n",
      "Epoch 96/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 4162.0428 - val_loss: 4599.2397\n",
      "Epoch 97/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 4078.5925 - val_loss: 4479.6104\n",
      "Epoch 98/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 4004.3066 - val_loss: 4381.1792\n",
      "Epoch 99/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3928.8384 - val_loss: 4289.3096\n",
      "Epoch 100/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3847.0037 - val_loss: 4196.1396\n",
      "Epoch 101/1000\n",
      "37114/37114 [==============================] - 1s 35us/sample - loss: 3768.4385 - val_loss: 4124.1562\n",
      "Epoch 102/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3694.3390 - val_loss: 4037.5391\n",
      "Epoch 103/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3617.0329 - val_loss: 3963.1931\n",
      "Epoch 104/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3540.3447 - val_loss: 3806.3105\n",
      "Epoch 105/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3467.9056 - val_loss: 3727.2581\n",
      "Epoch 106/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 3389.3916 - val_loss: 3619.7966\n",
      "Epoch 107/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 3315.0129 - val_loss: 3527.7080\n",
      "Epoch 108/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 3241.1518 - val_loss: 3404.9619\n",
      "Epoch 109/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 3167.9164 - val_loss: 3304.0491\n",
      "Epoch 110/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 3086.6233 - val_loss: 3207.0540\n",
      "Epoch 111/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 3026.9466 - val_loss: 3102.8462\n",
      "Epoch 112/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 2953.4085 - val_loss: 3029.3022\n",
      "Epoch 113/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2884.5992 - val_loss: 2928.0686\n",
      "Epoch 114/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2810.2907 - val_loss: 2862.9185\n",
      "Epoch 115/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2739.8723 - val_loss: 2796.3174\n",
      "Epoch 116/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 2672.6936 - val_loss: 2705.3281\n",
      "Epoch 117/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 2606.5990 - val_loss: 2611.5674\n",
      "Epoch 118/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 2538.1870 - val_loss: 2568.8789\n",
      "Epoch 119/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2467.4877 - val_loss: 2517.3643\n",
      "Epoch 120/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 2408.2650 - val_loss: 2434.0979\n",
      "Epoch 121/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 2343.3048 - val_loss: 2332.4233\n",
      "Epoch 122/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2277.1679 - val_loss: 2293.2490\n",
      "Epoch 123/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2211.7217 - val_loss: 2234.8708\n",
      "Epoch 124/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2153.2928 - val_loss: 2168.0144\n",
      "Epoch 125/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2097.9133 - val_loss: 2140.8330\n",
      "Epoch 126/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 2029.0736 - val_loss: 2074.4980\n",
      "Epoch 127/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1976.5383 - val_loss: 1984.1229\n",
      "Epoch 128/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 1922.5261 - val_loss: 1924.8091\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1862.2064 - val_loss: 1867.1879\n",
      "Epoch 130/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1801.6955 - val_loss: 1829.9093\n",
      "Epoch 131/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 1753.1152 - val_loss: 1773.6122\n",
      "Epoch 132/1000\n",
      "37114/37114 [==============================] - 1s 30us/sample - loss: 1693.4656 - val_loss: 1716.3048\n",
      "Epoch 133/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1645.9828 - val_loss: 1669.2806\n",
      "Epoch 134/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1591.4474 - val_loss: 1591.2419\n",
      "Epoch 135/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 1540.6478 - val_loss: 1552.2052\n",
      "Epoch 136/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 1493.7376 - val_loss: 1544.1411\n",
      "Epoch 137/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1445.9873 - val_loss: 1486.1101\n",
      "Epoch 138/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1398.6885 - val_loss: 1419.2330\n",
      "Epoch 139/1000\n",
      "37114/37114 [==============================] - 1s 36us/sample - loss: 1354.3228 - val_loss: 1379.5831\n",
      "Epoch 140/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1308.6142 - val_loss: 1328.4307\n",
      "Epoch 141/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1260.1958 - val_loss: 1287.8319\n",
      "Epoch 142/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 1221.5537 - val_loss: 1218.5204\n",
      "Epoch 143/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1183.1692 - val_loss: 1185.6660\n",
      "Epoch 144/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1141.4216 - val_loss: 1184.3984\n",
      "Epoch 145/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1100.2076 - val_loss: 1121.8174\n",
      "Epoch 146/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 1061.1643 - val_loss: 1099.2268\n",
      "Epoch 147/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 1024.8880 - val_loss: 1090.8909\n",
      "Epoch 148/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 991.4886 - val_loss: 1054.8038\n",
      "Epoch 149/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 952.2129 - val_loss: 1010.1606\n",
      "Epoch 150/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 917.9639 - val_loss: 966.2294\n",
      "Epoch 151/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 886.5208 - val_loss: 916.5621\n",
      "Epoch 152/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 855.8963 - val_loss: 911.6514\n",
      "Epoch 153/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 826.7787 - val_loss: 854.5370\n",
      "Epoch 154/1000\n",
      "37114/37114 [==============================] - 1s 35us/sample - loss: 794.5929 - val_loss: 823.4409\n",
      "Epoch 155/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 767.1152 - val_loss: 777.6643\n",
      "Epoch 156/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 736.3867 - val_loss: 778.2157\n",
      "Epoch 157/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 706.0839 - val_loss: 739.8542\n",
      "Epoch 158/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 683.3910 - val_loss: 723.0698\n",
      "Epoch 159/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 656.7390 - val_loss: 697.9135\n",
      "Epoch 160/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 633.4115 - val_loss: 661.8000\n",
      "Epoch 161/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 609.2964 - val_loss: 646.8099\n",
      "Epoch 162/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 582.7638 - val_loss: 630.3661\n",
      "Epoch 163/1000\n",
      "37114/37114 [==============================] - 1s 37us/sample - loss: 563.0010 - val_loss: 599.9673\n",
      "Epoch 164/1000\n",
      "37114/37114 [==============================] - 1s 36us/sample - loss: 537.3492 - val_loss: 576.7773\n",
      "Epoch 165/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 520.4809 - val_loss: 547.2157\n",
      "Epoch 166/1000\n",
      "37114/37114 [==============================] - 1s 28us/sample - loss: 499.8979 - val_loss: 533.2163\n",
      "Epoch 167/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 482.0427 - val_loss: 517.8357\n",
      "Epoch 168/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 462.3910 - val_loss: 495.7050\n",
      "Epoch 169/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 446.1631 - val_loss: 484.0812\n",
      "Epoch 170/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 425.9176 - val_loss: 471.9396\n",
      "Epoch 171/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 414.7316 - val_loss: 462.3634\n",
      "Epoch 172/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 398.3344 - val_loss: 458.6204\n",
      "Epoch 173/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 382.5820 - val_loss: 414.5596\n",
      "Epoch 174/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 364.1889 - val_loss: 428.4553\n",
      "Epoch 175/1000\n",
      "37114/37114 [==============================] - 2s 53us/sample - loss: 356.0240 - val_loss: 396.6884\n",
      "Epoch 176/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 342.9189 - val_loss: 385.2764\n",
      "Epoch 177/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 328.9241 - val_loss: 375.1960\n",
      "Epoch 178/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 315.4846 - val_loss: 365.9217\n",
      "Epoch 179/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 306.1086 - val_loss: 360.1336\n",
      "Epoch 180/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 294.7029 - val_loss: 346.5104\n",
      "Epoch 181/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 286.0385 - val_loss: 336.0666\n",
      "Epoch 182/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 275.8897 - val_loss: 327.8044\n",
      "Epoch 183/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 265.0365 - val_loss: 313.2762\n",
      "Epoch 184/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 256.7629 - val_loss: 297.8843\n",
      "Epoch 185/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 246.1277 - val_loss: 297.9072\n",
      "Epoch 186/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 238.3165 - val_loss: 298.0428\n",
      "Epoch 187/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 234.8125 - val_loss: 286.6631\n",
      "Epoch 188/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 222.3076 - val_loss: 286.0208\n",
      "Epoch 189/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 219.2850 - val_loss: 273.7991\n",
      "Epoch 190/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 211.1801 - val_loss: 269.9774\n",
      "Epoch 191/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 205.5455 - val_loss: 271.1964\n",
      "Epoch 192/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 197.8677 - val_loss: 258.5065\n",
      "Epoch 193/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 190.8444 - val_loss: 254.5795\n",
      "Epoch 194/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 186.8339 - val_loss: 252.0514\n",
      "Epoch 195/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 182.8672 - val_loss: 248.7223\n",
      "Epoch 196/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 175.9694 - val_loss: 240.3384\n",
      "Epoch 197/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 173.3739 - val_loss: 241.4523\n",
      "Epoch 198/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 169.5549 - val_loss: 241.4239\n",
      "Epoch 199/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 164.4853 - val_loss: 231.3334\n",
      "Epoch 200/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37114/37114 [==============================] - 1s 16us/sample - loss: 160.5303 - val_loss: 233.9610\n",
      "Epoch 201/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 155.9804 - val_loss: 224.0031\n",
      "Epoch 202/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 154.0284 - val_loss: 221.9346\n",
      "Epoch 203/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 148.8036 - val_loss: 222.9245\n",
      "Epoch 204/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 147.0738 - val_loss: 213.5718\n",
      "Epoch 205/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 144.3986 - val_loss: 220.4522\n",
      "Epoch 206/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 140.0955 - val_loss: 215.7166\n",
      "Epoch 207/1000\n",
      "37114/37114 [==============================] - 1s 28us/sample - loss: 138.6032 - val_loss: 208.0585\n",
      "Epoch 208/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 135.4370 - val_loss: 211.2802\n",
      "Epoch 209/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 133.7059 - val_loss: 216.2381\n",
      "Epoch 210/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 131.6267 - val_loss: 208.1868\n",
      "Epoch 211/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 128.4644 - val_loss: 205.4083\n",
      "Epoch 212/1000\n",
      "37114/37114 [==============================] - 1s 36us/sample - loss: 127.7284 - val_loss: 205.0984\n",
      "Epoch 213/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 124.6335 - val_loss: 204.0297\n",
      "Epoch 214/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 123.7784 - val_loss: 197.7048\n",
      "Epoch 215/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 122.7058 - val_loss: 201.9760\n",
      "Epoch 216/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 119.6270 - val_loss: 201.3075\n",
      "Epoch 217/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 120.9389 - val_loss: 195.8860\n",
      "Epoch 218/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 118.0179 - val_loss: 197.5391\n",
      "Epoch 219/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 116.6855 - val_loss: 193.5596\n",
      "Epoch 220/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 114.7753 - val_loss: 193.6217\n",
      "Epoch 221/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 113.0135 - val_loss: 196.7025\n",
      "Epoch 222/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 113.3027 - val_loss: 190.1084\n",
      "Epoch 223/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 111.3919 - val_loss: 192.9489\n",
      "Epoch 224/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 111.6391 - val_loss: 189.7003\n",
      "Epoch 225/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 109.8633 - val_loss: 189.6672\n",
      "Epoch 226/1000\n",
      "37114/37114 [==============================] - 1s 30us/sample - loss: 108.6308 - val_loss: 189.1000\n",
      "Epoch 227/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 107.6691 - val_loss: 190.0183\n",
      "Epoch 228/1000\n",
      "37114/37114 [==============================] - 1s 29us/sample - loss: 107.5668 - val_loss: 188.9161\n",
      "Epoch 229/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 106.8050 - val_loss: 191.8463\n",
      "Epoch 230/1000\n",
      "37114/37114 [==============================] - 1s 30us/sample - loss: 106.7077 - val_loss: 188.6207\n",
      "Epoch 231/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 105.5191 - val_loss: 186.3247\n",
      "Epoch 232/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 105.3244 - val_loss: 186.2837\n",
      "Epoch 233/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 103.7551 - val_loss: 189.4323\n",
      "Epoch 234/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 104.0577 - val_loss: 186.7250\n",
      "Epoch 235/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 103.7477 - val_loss: 187.4943\n",
      "Epoch 236/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 102.6510 - val_loss: 189.5060\n",
      "Epoch 237/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 104.9813 - val_loss: 187.3994\n",
      "Epoch 238/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 102.2159 - val_loss: 185.9509\n",
      "Epoch 239/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 102.3097 - val_loss: 186.8225\n",
      "Epoch 240/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 100.0348 - val_loss: 186.9399\n",
      "Epoch 241/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 101.5773 - val_loss: 185.5470\n",
      "Epoch 242/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 99.1919 - val_loss: 186.0795\n",
      "Epoch 243/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 99.7100 - val_loss: 186.8286\n",
      "Epoch 244/1000\n",
      "37114/37114 [==============================] - 1s 28us/sample - loss: 100.0276 - val_loss: 184.2496\n",
      "Epoch 245/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 99.6839 - val_loss: 185.0302\n",
      "Epoch 246/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 98.4039 - val_loss: 184.5869\n",
      "Epoch 247/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 99.4247 - val_loss: 186.0297\n",
      "Epoch 248/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 97.2154 - val_loss: 185.5267\n",
      "Epoch 249/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 98.3479 - val_loss: 183.7608\n",
      "Epoch 250/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 98.6790 - val_loss: 186.6074\n",
      "Epoch 251/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 97.7336 - val_loss: 185.6464\n",
      "Epoch 252/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 98.0825 - val_loss: 184.4270\n",
      "Epoch 253/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 97.9200 - val_loss: 184.7485\n",
      "Epoch 254/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 96.6845 - val_loss: 184.5862\n",
      "Epoch 255/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 97.1584 - val_loss: 184.8454\n",
      "Epoch 256/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 96.0243 - val_loss: 183.8658\n",
      "Epoch 257/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 96.5055 - val_loss: 182.6952\n",
      "Epoch 258/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 95.6765 - val_loss: 182.2207\n",
      "Epoch 259/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 94.7563 - val_loss: 183.2667\n",
      "Epoch 260/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 95.4422 - val_loss: 182.1680\n",
      "Epoch 261/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 95.5805 - val_loss: 182.7409\n",
      "Epoch 262/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 92.7315 - val_loss: 182.1751\n",
      "Epoch 263/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 95.0792 - val_loss: 181.9418\n",
      "Epoch 264/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 93.8931 - val_loss: 181.7913\n",
      "Epoch 265/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 93.2088 - val_loss: 180.4854\n",
      "Epoch 266/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 94.9079 - val_loss: 181.6273\n",
      "Epoch 267/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.2637 - val_loss: 183.3008\n",
      "Epoch 268/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.5549 - val_loss: 181.1613\n",
      "Epoch 269/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 94.4876 - val_loss: 180.5853\n",
      "Epoch 270/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.9240 - val_loss: 181.4186\n",
      "Epoch 271/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.4472 - val_loss: 180.5811\n",
      "Epoch 272/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 94.4177 - val_loss: 180.2328\n",
      "Epoch 273/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 91.8138 - val_loss: 182.4481\n",
      "Epoch 274/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 91.6580 - val_loss: 181.1868\n",
      "Epoch 275/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.2734 - val_loss: 180.7636\n",
      "Epoch 276/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 92.5864 - val_loss: 180.5274\n",
      "Epoch 277/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 91.1325 - val_loss: 179.8173\n",
      "Epoch 278/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 93.6421 - val_loss: 180.3256\n",
      "Epoch 279/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 91.9146 - val_loss: 179.9338\n",
      "Epoch 280/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.6500 - val_loss: 180.4511\n",
      "Epoch 281/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 91.0205 - val_loss: 180.5233\n",
      "Epoch 282/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.4460 - val_loss: 180.3031\n",
      "Epoch 283/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 92.6944 - val_loss: 180.3873\n",
      "Epoch 284/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 90.9213 - val_loss: 179.1686\n",
      "Epoch 285/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.0182 - val_loss: 179.8377\n",
      "Epoch 286/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.5565 - val_loss: 180.1582\n",
      "Epoch 287/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 92.1747 - val_loss: 180.1520\n",
      "Epoch 288/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.5124 - val_loss: 179.2168\n",
      "Epoch 289/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.9633 - val_loss: 180.0424\n",
      "Epoch 290/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 89.8274 - val_loss: 178.7212\n",
      "Epoch 291/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.5378 - val_loss: 179.4558\n",
      "Epoch 292/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.9977 - val_loss: 179.7405\n",
      "Epoch 293/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.4213 - val_loss: 178.9679\n",
      "Epoch 294/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 90.3676 - val_loss: 179.6832\n",
      "Epoch 295/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.7625 - val_loss: 180.8094\n",
      "Epoch 296/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.6662 - val_loss: 179.5473\n",
      "Epoch 297/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.7894 - val_loss: 180.0676\n",
      "Epoch 298/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.0983 - val_loss: 180.9338\n",
      "Epoch 299/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.1215 - val_loss: 181.0271\n",
      "Epoch 300/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.5488 - val_loss: 179.7722\n",
      "Epoch 301/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.5575 - val_loss: 179.0352\n",
      "Epoch 302/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.8318 - val_loss: 178.9321\n",
      "Epoch 303/1000\n",
      "37114/37114 [==============================] - 1s 29us/sample - loss: 88.1811 - val_loss: 177.7947\n",
      "Epoch 304/1000\n",
      "37114/37114 [==============================] - 1s 27us/sample - loss: 88.8953 - val_loss: 177.6660\n",
      "Epoch 305/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.0677 - val_loss: 180.3041\n",
      "Epoch 306/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.3401 - val_loss: 180.1420\n",
      "Epoch 307/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.5557 - val_loss: 180.6450\n",
      "Epoch 308/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.0440 - val_loss: 179.4569\n",
      "Epoch 309/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.7720 - val_loss: 179.0510\n",
      "Epoch 310/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 89.1473 - val_loss: 180.6786\n",
      "Epoch 311/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.2418 - val_loss: 179.8558\n",
      "Epoch 312/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 88.2386 - val_loss: 180.1591\n",
      "Epoch 313/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.6531 - val_loss: 178.2003\n",
      "Epoch 314/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.7435 - val_loss: 178.9073\n",
      "Epoch 315/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 87.0853 - val_loss: 179.6621\n",
      "Epoch 316/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.6797 - val_loss: 179.4314\n",
      "Epoch 317/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 87.7605 - val_loss: 180.9588\n",
      "Epoch 318/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.7167 - val_loss: 182.5554\n",
      "Epoch 319/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.4730 - val_loss: 179.8846\n",
      "Epoch 320/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.6103 - val_loss: 179.0747\n",
      "Epoch 321/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 84.6772 - val_loss: 178.1570\n",
      "Epoch 322/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 86.3422 - val_loss: 179.6674\n",
      "Epoch 323/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.8303 - val_loss: 180.2898\n",
      "Epoch 324/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 84.8382 - val_loss: 179.4109\n",
      "Epoch 325/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 84.6513 - val_loss: 178.9110\n",
      "Epoch 326/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.0738 - val_loss: 178.7912\n",
      "Epoch 327/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.7134 - val_loss: 178.6348\n",
      "Epoch 328/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.1032 - val_loss: 178.6127\n",
      "Epoch 329/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 84.1179 - val_loss: 178.6909\n",
      "Epoch 330/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.0361 - val_loss: 179.2953\n",
      "Epoch 331/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 85.2356 - val_loss: 179.9028\n",
      "Epoch 332/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.5374 - val_loss: 179.8208\n",
      "Epoch 333/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.9703 - val_loss: 180.9363\n",
      "Epoch 334/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.5728 - val_loss: 179.7425\n",
      "Epoch 335/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.4550 - val_loss: 180.5051\n",
      "Epoch 336/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 84.7555 - val_loss: 180.7972\n",
      "Epoch 337/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.4296 - val_loss: 179.8825\n",
      "Epoch 338/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.6203 - val_loss: 179.7413\n",
      "Epoch 339/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.6255 - val_loss: 178.9459\n",
      "Epoch 340/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 82.4220 - val_loss: 178.8016\n",
      "Epoch 341/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.9543 - val_loss: 178.5915\n",
      "Epoch 342/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.2025 - val_loss: 179.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.4937 - val_loss: 179.2845\n",
      "Epoch 344/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.3850 - val_loss: 179.9791\n",
      "Epoch 345/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 83.3077 - val_loss: 180.0987\n",
      "Epoch 346/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 82.0340 - val_loss: 178.5516\n",
      "Epoch 347/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 82.7369 - val_loss: 179.0745\n",
      "Epoch 348/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 81.9334 - val_loss: 178.8486\n",
      "Epoch 349/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 81.4165 - val_loss: 178.8419\n",
      "Epoch 350/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 81.8251 - val_loss: 177.8901\n",
      "Epoch 351/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.4613 - val_loss: 177.7716\n",
      "Epoch 352/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 80.4414 - val_loss: 178.5839\n",
      "Epoch 353/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.7781 - val_loss: 178.7176\n",
      "Epoch 354/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 81.5599 - val_loss: 178.7851\n",
      "Epoch 355/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 80.8647 - val_loss: 179.0435\n",
      "Epoch 356/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 81.1503 - val_loss: 178.8930\n",
      "Epoch 357/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.2308 - val_loss: 178.3226\n",
      "Epoch 358/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.6664 - val_loss: 177.7977\n",
      "Epoch 359/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 81.2867 - val_loss: 177.4149\n",
      "Epoch 360/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.5175 - val_loss: 177.4437\n",
      "Epoch 361/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.1007 - val_loss: 178.5505\n",
      "Epoch 362/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 79.5055 - val_loss: 178.5398\n",
      "Epoch 363/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 79.4882 - val_loss: 178.5310\n",
      "Epoch 364/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.2780 - val_loss: 179.0395\n",
      "Epoch 365/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 79.9511 - val_loss: 178.5186\n",
      "Epoch 366/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 78.2162 - val_loss: 178.8353\n",
      "Epoch 367/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 80.6155 - val_loss: 178.5267\n",
      "Epoch 368/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 79.7287 - val_loss: 177.9726\n",
      "Epoch 369/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 78.9046 - val_loss: 177.2418\n",
      "Epoch 370/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 78.9180 - val_loss: 177.0058\n",
      "Epoch 371/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 79.6440 - val_loss: 177.8385\n",
      "Epoch 372/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 78.4862 - val_loss: 177.4912\n",
      "Epoch 373/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 78.8762 - val_loss: 177.6800\n",
      "Epoch 374/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 78.0827 - val_loss: 177.5621\n",
      "Epoch 375/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 78.1952 - val_loss: 176.6682\n",
      "Epoch 376/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.6473 - val_loss: 177.3385\n",
      "Epoch 377/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.3175 - val_loss: 177.5747\n",
      "Epoch 378/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 78.4885 - val_loss: 177.2008\n",
      "Epoch 379/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.5421 - val_loss: 177.2265\n",
      "Epoch 380/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 78.2844 - val_loss: 176.6252\n",
      "Epoch 381/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 78.9357 - val_loss: 176.5617\n",
      "Epoch 382/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 76.7708 - val_loss: 176.0666\n",
      "Epoch 383/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.5552 - val_loss: 176.6311\n",
      "Epoch 384/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 77.6748 - val_loss: 177.4341\n",
      "Epoch 385/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.2305 - val_loss: 177.3833\n",
      "Epoch 386/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.3695 - val_loss: 177.5936\n",
      "Epoch 387/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.3057 - val_loss: 178.3718\n",
      "Epoch 388/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 76.8674 - val_loss: 177.5929\n",
      "Epoch 389/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.4203 - val_loss: 176.2760\n",
      "Epoch 390/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 77.2196 - val_loss: 177.2876\n",
      "Epoch 391/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.9862 - val_loss: 176.7990\n",
      "Epoch 392/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 76.7248 - val_loss: 175.3003\n",
      "Epoch 393/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.8122 - val_loss: 176.0604\n",
      "Epoch 394/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 75.5301 - val_loss: 177.9387\n",
      "Epoch 395/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 74.7130 - val_loss: 177.1342\n",
      "Epoch 396/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.4627 - val_loss: 177.4588\n",
      "Epoch 397/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.0918 - val_loss: 176.1003\n",
      "Epoch 398/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 75.7407 - val_loss: 176.8208\n",
      "Epoch 399/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 75.9996 - val_loss: 176.9674\n",
      "Epoch 400/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 74.9572 - val_loss: 176.4492\n",
      "Epoch 401/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 76.4952 - val_loss: 176.4730\n",
      "Epoch 402/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 75.0807 - val_loss: 175.2264\n",
      "Epoch 403/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 75.7863 - val_loss: 174.9757\n",
      "Epoch 404/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 75.1232 - val_loss: 175.3663\n",
      "Epoch 405/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 75.4208 - val_loss: 175.7960\n",
      "Epoch 406/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 74.5296 - val_loss: 175.5657\n",
      "Epoch 407/1000\n",
      "37114/37114 [==============================] - 1s 29us/sample - loss: 74.8150 - val_loss: 174.8571\n",
      "Epoch 408/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.9813 - val_loss: 175.3210\n",
      "Epoch 409/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.9387 - val_loss: 175.6354\n",
      "Epoch 410/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 74.3954 - val_loss: 175.4993\n",
      "Epoch 411/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.8413 - val_loss: 176.3621\n",
      "Epoch 412/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.4214 - val_loss: 176.2897\n",
      "Epoch 413/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 74.0059 - val_loss: 176.8546\n",
      "Epoch 414/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.9599 - val_loss: 176.6966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.0545 - val_loss: 177.1065\n",
      "Epoch 416/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.3160 - val_loss: 176.3514\n",
      "Epoch 417/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.7650 - val_loss: 176.5570\n",
      "Epoch 418/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 74.5692 - val_loss: 176.9993\n",
      "Epoch 419/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 74.7171 - val_loss: 175.5775\n",
      "Epoch 420/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.0185 - val_loss: 175.1100\n",
      "Epoch 421/1000\n",
      "37114/37114 [==============================] - 1s 33us/sample - loss: 72.0696 - val_loss: 174.6667\n",
      "Epoch 422/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.4425 - val_loss: 175.7387\n",
      "Epoch 423/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.9396 - val_loss: 176.1330\n",
      "Epoch 424/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.1307 - val_loss: 175.5770\n",
      "Epoch 425/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.7541 - val_loss: 176.0170\n",
      "Epoch 426/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.6687 - val_loss: 174.8225\n",
      "Epoch 427/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.8001 - val_loss: 175.8023\n",
      "Epoch 428/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 73.3569 - val_loss: 176.5541\n",
      "Epoch 429/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.4344 - val_loss: 177.8031\n",
      "Epoch 430/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.9518 - val_loss: 177.7828\n",
      "Epoch 431/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.9824 - val_loss: 176.5695\n",
      "Epoch 432/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.1177 - val_loss: 176.7626\n",
      "Epoch 433/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 71.0276 - val_loss: 176.4378\n",
      "Epoch 434/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.2869 - val_loss: 177.3593\n",
      "Epoch 435/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 72.5314 - val_loss: 176.0005\n",
      "Epoch 436/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.2830 - val_loss: 175.7858\n",
      "Epoch 437/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.0603 - val_loss: 178.0955\n",
      "Epoch 438/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.6938 - val_loss: 177.0759\n",
      "Epoch 439/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.7063 - val_loss: 176.6051\n",
      "Epoch 440/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.9141 - val_loss: 176.7645\n",
      "Epoch 441/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 71.9141 - val_loss: 175.7203\n",
      "Epoch 442/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.4081 - val_loss: 177.1640\n",
      "Epoch 443/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 71.2891 - val_loss: 175.7842\n",
      "Epoch 444/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.2578 - val_loss: 178.0532\n",
      "Epoch 445/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.6357 - val_loss: 177.2229\n",
      "Epoch 446/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.7355 - val_loss: 178.6656\n",
      "Epoch 447/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.4888 - val_loss: 178.2371\n",
      "Epoch 448/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 69.9170 - val_loss: 177.4932\n",
      "Epoch 449/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.5774 - val_loss: 178.0051\n",
      "Epoch 450/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.2828 - val_loss: 178.7216\n",
      "Epoch 451/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.6039 - val_loss: 177.8792\n",
      "Epoch 452/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.8309 - val_loss: 176.8049\n",
      "Epoch 453/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.0675 - val_loss: 176.4423\n",
      "Epoch 454/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.5857 - val_loss: 176.9944\n",
      "Epoch 455/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.0725 - val_loss: 177.4868\n",
      "Epoch 456/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 70.6409 - val_loss: 176.8575\n",
      "Epoch 457/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.3021 - val_loss: 176.5006\n",
      "Epoch 458/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.4775 - val_loss: 176.3343\n",
      "Epoch 459/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.2618 - val_loss: 175.7968\n",
      "Epoch 460/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 68.4467 - val_loss: 175.2722\n",
      "Epoch 461/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 68.2818 - val_loss: 176.5450\n",
      "Epoch 462/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.8090 - val_loss: 177.9043\n",
      "Epoch 463/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.6467 - val_loss: 176.5405\n",
      "Epoch 464/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.8522 - val_loss: 177.6564\n",
      "Epoch 465/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.8661 - val_loss: 175.7767\n",
      "Epoch 466/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 69.2882 - val_loss: 176.4904\n",
      "Epoch 467/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.7039 - val_loss: 176.8057\n",
      "Epoch 468/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 69.1345 - val_loss: 176.3712\n",
      "Epoch 469/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.2737 - val_loss: 176.1879\n",
      "Epoch 470/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.3551 - val_loss: 175.9388\n",
      "Epoch 471/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.9645 - val_loss: 175.1037\n",
      "Epoch 472/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.8314 - val_loss: 175.1455\n",
      "Epoch 473/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.7928 - val_loss: 176.5737\n",
      "Epoch 474/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.8221 - val_loss: 176.3011\n",
      "Epoch 475/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.0529 - val_loss: 175.8578\n",
      "Epoch 476/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.7982 - val_loss: 176.0057\n",
      "Epoch 477/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.3604 - val_loss: 176.0851\n",
      "Epoch 478/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.8063 - val_loss: 175.6441\n",
      "Epoch 479/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.0568 - val_loss: 175.9574\n",
      "Epoch 480/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 67.2463 - val_loss: 174.4617\n",
      "Epoch 481/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 68.2004 - val_loss: 175.1310\n",
      "Epoch 482/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.4475 - val_loss: 175.9745\n",
      "Epoch 483/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.7917 - val_loss: 176.0945\n",
      "Epoch 484/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 67.3757 - val_loss: 175.4977\n",
      "Epoch 485/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 66.8462 - val_loss: 176.0782\n",
      "Epoch 486/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 66.4804 - val_loss: 176.2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.1532 - val_loss: 176.0934\n",
      "Epoch 488/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.9204 - val_loss: 175.9462\n",
      "Epoch 489/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.7651 - val_loss: 175.2176\n",
      "Epoch 490/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.1191 - val_loss: 175.1535\n",
      "Epoch 491/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 66.2240 - val_loss: 176.4120\n",
      "Epoch 492/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.4880 - val_loss: 176.4063\n",
      "Epoch 493/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.1692 - val_loss: 175.6827\n",
      "Epoch 494/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.8924 - val_loss: 175.6837\n",
      "Epoch 495/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.9869 - val_loss: 176.1679\n",
      "Epoch 496/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.1764 - val_loss: 176.3552\n",
      "Epoch 497/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.8517 - val_loss: 176.9211\n",
      "Epoch 498/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.1818 - val_loss: 176.2673\n",
      "Epoch 499/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 65.1486 - val_loss: 176.4313\n",
      "Epoch 500/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 66.1780 - val_loss: 175.8347\n",
      "Epoch 501/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.5763 - val_loss: 175.1956\n",
      "Epoch 502/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.3995 - val_loss: 175.3439\n",
      "Epoch 503/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.2680 - val_loss: 176.0473\n",
      "Epoch 504/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.1001 - val_loss: 175.0279\n",
      "Epoch 505/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.7477 - val_loss: 176.4419\n",
      "Epoch 506/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.3606 - val_loss: 175.4016\n",
      "Epoch 507/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.2467 - val_loss: 174.8503\n",
      "Epoch 508/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.9152 - val_loss: 174.7722\n",
      "Epoch 509/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.5192 - val_loss: 175.0834\n",
      "Epoch 510/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.4611 - val_loss: 175.6633\n",
      "Epoch 511/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 64.4682 - val_loss: 173.6875\n",
      "Epoch 512/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 64.8925 - val_loss: 174.6523\n",
      "Epoch 513/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 65.7584 - val_loss: 174.4652\n",
      "Epoch 514/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.4446 - val_loss: 174.4601\n",
      "Epoch 515/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.3501 - val_loss: 174.8043\n",
      "Epoch 516/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.1136 - val_loss: 175.8091\n",
      "Epoch 517/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.5615 - val_loss: 174.5547\n",
      "Epoch 518/1000\n",
      "37114/37114 [==============================] - 0s 13us/sample - loss: 65.1765 - val_loss: 174.5975\n",
      "Epoch 519/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.6144 - val_loss: 174.6004\n",
      "Epoch 520/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.0721 - val_loss: 175.0763\n",
      "Epoch 521/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 64.6399 - val_loss: 174.4075\n",
      "Epoch 522/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.7472 - val_loss: 174.0872\n",
      "Epoch 523/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.8002 - val_loss: 173.8989\n",
      "Epoch 524/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.2578 - val_loss: 174.1239\n",
      "Epoch 525/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.8850 - val_loss: 175.3371\n",
      "Epoch 526/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 63.7850 - val_loss: 175.0001\n",
      "Epoch 527/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.4919 - val_loss: 174.5852\n",
      "Epoch 528/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.6723 - val_loss: 175.2637\n",
      "Epoch 529/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 63.0402 - val_loss: 174.9562\n",
      "Epoch 530/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.3958 - val_loss: 174.1471\n",
      "Epoch 531/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.7002 - val_loss: 174.6316\n",
      "Epoch 532/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.0580 - val_loss: 174.4086\n",
      "Epoch 533/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 63.1963 - val_loss: 174.8474\n",
      "Epoch 534/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.7016 - val_loss: 176.1882\n",
      "Epoch 535/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.9790 - val_loss: 175.2020\n",
      "Epoch 536/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.3607 - val_loss: 175.1941\n",
      "Epoch 537/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.4414 - val_loss: 175.3576\n",
      "Epoch 538/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.2649 - val_loss: 175.4230\n",
      "Epoch 539/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.9353 - val_loss: 175.7669\n",
      "Epoch 540/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.6377 - val_loss: 174.9718\n",
      "Epoch 541/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.5741 - val_loss: 174.7859\n",
      "Epoch 542/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.6429 - val_loss: 174.9782\n",
      "Epoch 543/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.5806 - val_loss: 174.6182\n",
      "Epoch 544/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.7457 - val_loss: 174.6066\n",
      "Epoch 545/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.4460 - val_loss: 175.2728\n",
      "Epoch 546/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.0570 - val_loss: 176.0756\n",
      "Epoch 547/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 62.7883 - val_loss: 176.0350\n",
      "Epoch 548/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.6384 - val_loss: 175.8743\n",
      "Epoch 549/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.5428 - val_loss: 174.5252\n",
      "Epoch 550/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.3131 - val_loss: 175.5988\n",
      "Epoch 551/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.5025 - val_loss: 176.3781\n",
      "Epoch 552/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 61.6314 - val_loss: 175.6737\n",
      "Epoch 553/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.5474 - val_loss: 174.6075\n",
      "Epoch 554/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 60.6935 - val_loss: 173.4213\n",
      "Epoch 555/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.0025 - val_loss: 174.0267\n",
      "Epoch 556/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.3442 - val_loss: 175.0334\n",
      "Epoch 557/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.6803 - val_loss: 174.6984\n",
      "Epoch 558/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.2009 - val_loss: 175.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 559/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.7815 - val_loss: 174.1736\n",
      "Epoch 560/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.0902 - val_loss: 174.5463\n",
      "Epoch 561/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.5492 - val_loss: 174.3549\n",
      "Epoch 562/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.0136 - val_loss: 174.1733\n",
      "Epoch 563/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.8710 - val_loss: 174.7556\n",
      "Epoch 564/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 60.7456 - val_loss: 173.8392\n",
      "Epoch 565/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.2316 - val_loss: 173.7925\n",
      "Epoch 566/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.2916 - val_loss: 173.8690\n",
      "Epoch 567/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.6579 - val_loss: 174.4451\n",
      "Epoch 568/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.7565 - val_loss: 174.4131\n",
      "Epoch 569/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.0340 - val_loss: 174.0625\n",
      "Epoch 570/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 61.1174 - val_loss: 174.0297\n",
      "Epoch 571/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 59.5534 - val_loss: 175.1057\n",
      "Epoch 572/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 60.1714 - val_loss: 175.1750\n",
      "Epoch 573/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.8169 - val_loss: 175.4503\n",
      "Epoch 574/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.9230 - val_loss: 175.0367\n",
      "Epoch 575/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.4780 - val_loss: 175.3523\n",
      "Epoch 576/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.1305 - val_loss: 175.0900\n",
      "Epoch 577/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.9358 - val_loss: 174.6155\n",
      "Epoch 578/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.4359 - val_loss: 174.7010\n",
      "Epoch 579/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.1195 - val_loss: 174.7057\n",
      "Epoch 580/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.3777 - val_loss: 175.9472\n",
      "Epoch 581/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.9677 - val_loss: 175.4604\n",
      "Epoch 582/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.0593 - val_loss: 175.9267\n",
      "Epoch 583/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.4907 - val_loss: 176.1015\n",
      "Epoch 584/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.4202 - val_loss: 175.6158\n",
      "Epoch 585/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.8386 - val_loss: 175.2026\n",
      "Epoch 586/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.3796 - val_loss: 175.2404\n",
      "Epoch 587/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.1443 - val_loss: 175.2444\n",
      "Epoch 588/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 58.3391 - val_loss: 174.6044\n",
      "Epoch 589/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.3963 - val_loss: 174.3705\n",
      "Epoch 590/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.4042 - val_loss: 176.3295\n",
      "Epoch 591/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.8669 - val_loss: 175.7299\n",
      "Epoch 592/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.9117 - val_loss: 175.1058\n",
      "Epoch 593/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.5376 - val_loss: 175.8150\n",
      "Epoch 594/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1702 - val_loss: 176.2319\n",
      "Epoch 595/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.0660 - val_loss: 175.8832\n",
      "Epoch 596/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.5992 - val_loss: 174.8404\n",
      "Epoch 597/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.3385 - val_loss: 175.7347\n",
      "Epoch 598/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.8811 - val_loss: 176.2482\n",
      "Epoch 599/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.0217 - val_loss: 175.3675\n",
      "Epoch 600/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.0271 - val_loss: 174.8463\n",
      "Epoch 601/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.5056 - val_loss: 174.6793\n",
      "Epoch 602/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.2261 - val_loss: 174.5971\n",
      "Epoch 603/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.6284 - val_loss: 174.1743\n",
      "Epoch 604/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.9359 - val_loss: 175.4469\n",
      "Epoch 605/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.9936 - val_loss: 175.1698\n",
      "Epoch 606/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.2219 - val_loss: 174.9635\n",
      "Epoch 607/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.7943 - val_loss: 174.7290\n",
      "Epoch 608/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.7372 - val_loss: 175.6215\n",
      "Epoch 609/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.3276 - val_loss: 174.9178\n",
      "Epoch 610/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.4976 - val_loss: 176.4043\n",
      "Epoch 611/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.3106 - val_loss: 177.1571\n",
      "Epoch 612/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 59.0177 - val_loss: 176.3879\n",
      "Epoch 613/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.0008 - val_loss: 175.6419\n",
      "Epoch 614/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1000 - val_loss: 174.9433\n",
      "Epoch 615/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.0199 - val_loss: 174.7163\n",
      "Epoch 616/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.8680 - val_loss: 173.7730\n",
      "Epoch 617/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.3978 - val_loss: 173.8518\n",
      "Epoch 618/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 58.6396 - val_loss: 173.9793\n",
      "Epoch 619/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 58.0964 - val_loss: 174.1539\n",
      "Epoch 620/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.6709 - val_loss: 175.0001\n",
      "Epoch 621/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.7769 - val_loss: 175.3605\n",
      "Epoch 622/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.8867 - val_loss: 175.9781\n",
      "Epoch 623/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 57.5699 - val_loss: 175.8307\n",
      "Epoch 624/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.7210 - val_loss: 175.7128\n",
      "Epoch 625/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.7803 - val_loss: 175.8770\n",
      "Epoch 626/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.6395 - val_loss: 176.2425\n",
      "Epoch 627/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1262 - val_loss: 175.4552\n",
      "Epoch 628/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1028 - val_loss: 174.6987\n",
      "Epoch 629/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.7168 - val_loss: 175.8905\n",
      "Epoch 630/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1475 - val_loss: 175.2291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 631/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 56.0682 - val_loss: 174.7413\n",
      "Epoch 632/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.0814 - val_loss: 175.5967\n",
      "Epoch 633/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.9908 - val_loss: 176.3200\n",
      "Epoch 634/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.7328 - val_loss: 175.2302\n",
      "Epoch 635/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9471 - val_loss: 174.7107\n",
      "Epoch 636/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.9241 - val_loss: 174.8188\n",
      "Epoch 637/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 57.1084 - val_loss: 174.9027\n",
      "Epoch 638/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.3185 - val_loss: 174.7699\n",
      "Epoch 639/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.0700 - val_loss: 174.5416\n",
      "Epoch 640/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.5389 - val_loss: 173.8790\n",
      "Epoch 641/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 55.5814 - val_loss: 173.1843\n",
      "Epoch 642/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.1019 - val_loss: 174.0645\n",
      "Epoch 643/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 57.6631 - val_loss: 175.4583\n",
      "Epoch 644/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.0522 - val_loss: 175.7953\n",
      "Epoch 645/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 56.6143 - val_loss: 175.6868\n",
      "Epoch 646/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9189 - val_loss: 174.7827\n",
      "Epoch 647/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 57.6674 - val_loss: 174.5331\n",
      "Epoch 648/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9385 - val_loss: 174.2778\n",
      "Epoch 649/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.6440 - val_loss: 174.0298\n",
      "Epoch 650/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 57.7753 - val_loss: 174.0888\n",
      "Epoch 651/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9896 - val_loss: 174.5183\n",
      "Epoch 652/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.1840 - val_loss: 173.8901\n",
      "Epoch 653/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.8089 - val_loss: 174.2494\n",
      "Epoch 654/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.4434 - val_loss: 174.9050\n",
      "Epoch 655/1000\n",
      "37114/37114 [==============================] - 0s 13us/sample - loss: 57.0232 - val_loss: 174.1284\n",
      "Epoch 656/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.1415 - val_loss: 174.9836\n",
      "Epoch 657/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.2757 - val_loss: 175.4679\n",
      "Epoch 658/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.1507 - val_loss: 174.6723\n",
      "Epoch 659/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.1305 - val_loss: 175.2919\n",
      "Epoch 660/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.5362 - val_loss: 175.6908\n",
      "Epoch 661/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 56.3164 - val_loss: 174.5801\n",
      "Epoch 662/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.0503 - val_loss: 174.3790\n",
      "Epoch 663/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.8964 - val_loss: 174.6107\n",
      "Epoch 664/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.6917 - val_loss: 174.8092\n",
      "Epoch 665/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.0020 - val_loss: 174.3091\n",
      "Epoch 666/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 55.1139 - val_loss: 174.0791\n",
      "Epoch 667/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.7451 - val_loss: 174.2756\n",
      "Epoch 668/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.8733 - val_loss: 175.2070\n",
      "Epoch 669/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9441 - val_loss: 174.7769\n",
      "Epoch 670/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 55.5133 - val_loss: 174.5921\n",
      "Epoch 671/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.5688 - val_loss: 174.6787\n",
      "Epoch 672/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.5995 - val_loss: 175.3755\n",
      "Epoch 673/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 56.0229 - val_loss: 175.0938\n",
      "Epoch 674/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.9376 - val_loss: 175.4435\n",
      "Epoch 675/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.7957 - val_loss: 175.8159\n",
      "Epoch 676/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 55.6634 - val_loss: 175.5958\n",
      "Epoch 677/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7162 - val_loss: 175.5590\n",
      "Epoch 678/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 55.5962 - val_loss: 174.7732\n",
      "Epoch 679/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 55.5644 - val_loss: 175.1857\n",
      "Epoch 680/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.4612 - val_loss: 175.6767\n",
      "Epoch 681/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1258 - val_loss: 175.0933\n",
      "Epoch 682/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7651 - val_loss: 174.7182\n",
      "Epoch 683/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 54.3979 - val_loss: 175.2446\n",
      "Epoch 684/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7342 - val_loss: 175.1122\n",
      "Epoch 685/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.8670 - val_loss: 174.7994\n",
      "Epoch 686/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 55.0961 - val_loss: 174.8922\n",
      "Epoch 687/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.8408 - val_loss: 174.8960\n",
      "Epoch 688/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.9618 - val_loss: 175.2373\n",
      "Epoch 689/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.3825 - val_loss: 174.9627\n",
      "Epoch 690/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.9323 - val_loss: 174.5282\n",
      "Epoch 691/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.4173 - val_loss: 175.3256\n",
      "Epoch 692/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.1255 - val_loss: 175.9612\n",
      "Epoch 693/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.8670 - val_loss: 176.0184\n",
      "Epoch 694/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.1194 - val_loss: 174.1349\n",
      "Epoch 695/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.0017 - val_loss: 174.0697\n",
      "Epoch 696/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 55.8155 - val_loss: 174.3600\n",
      "Epoch 697/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7711 - val_loss: 175.0102\n",
      "Epoch 698/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 55.0130 - val_loss: 175.0171\n",
      "Epoch 699/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.6513 - val_loss: 174.2317\n",
      "Epoch 700/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.4648 - val_loss: 174.7592\n",
      "Epoch 701/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.6978 - val_loss: 175.4407\n",
      "Epoch 702/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5566 - val_loss: 175.7469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3827 - val_loss: 175.7885\n",
      "Epoch 704/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5464 - val_loss: 174.2402\n",
      "Epoch 705/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.6035 - val_loss: 173.8846\n",
      "Epoch 706/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 54.1874 - val_loss: 174.6767\n",
      "Epoch 707/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3679 - val_loss: 174.3296\n",
      "Epoch 708/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.2843 - val_loss: 174.8938\n",
      "Epoch 709/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5489 - val_loss: 175.0929\n",
      "Epoch 710/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.1046 - val_loss: 174.1856\n",
      "Epoch 711/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5388 - val_loss: 174.8652\n",
      "Epoch 712/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7016 - val_loss: 174.9349\n",
      "Epoch 713/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3977 - val_loss: 174.1235\n",
      "Epoch 714/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1700 - val_loss: 173.3420\n",
      "Epoch 715/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8463 - val_loss: 173.4317\n",
      "Epoch 716/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3934 - val_loss: 173.9539\n",
      "Epoch 717/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5995 - val_loss: 174.1689\n",
      "Epoch 718/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.0742 - val_loss: 174.7371\n",
      "Epoch 719/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.8997 - val_loss: 174.1073\n",
      "Epoch 720/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 54.5355 - val_loss: 173.1578\n",
      "Epoch 721/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 54.2977 - val_loss: 173.8226\n",
      "Epoch 722/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7810 - val_loss: 174.7797\n",
      "Epoch 723/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5646 - val_loss: 175.0701\n",
      "Epoch 724/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 55.2007 - val_loss: 175.8999\n",
      "Epoch 725/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.8171 - val_loss: 174.4104\n",
      "Epoch 726/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.6092 - val_loss: 173.9694\n",
      "Epoch 727/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1905 - val_loss: 174.6723\n",
      "Epoch 728/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3948 - val_loss: 174.2232\n",
      "Epoch 729/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5363 - val_loss: 174.4399\n",
      "Epoch 730/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.4506 - val_loss: 173.8953\n",
      "Epoch 731/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1938 - val_loss: 173.6051\n",
      "Epoch 732/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5552 - val_loss: 174.1495\n",
      "Epoch 733/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5509 - val_loss: 174.2854\n",
      "Epoch 734/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3460 - val_loss: 173.5456\n",
      "Epoch 735/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.4937 - val_loss: 173.2818\n",
      "Epoch 736/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6621 - val_loss: 173.5175\n",
      "Epoch 737/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 53.6376 - val_loss: 172.8917\n",
      "Epoch 738/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 54.4783 - val_loss: 172.8978\n",
      "Epoch 739/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 55.1304 - val_loss: 172.8823\n",
      "Epoch 740/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6711 - val_loss: 173.2200\n",
      "Epoch 741/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.9829 - val_loss: 173.6650\n",
      "Epoch 742/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7216 - val_loss: 174.0602\n",
      "Epoch 743/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3072 - val_loss: 174.5337\n",
      "Epoch 744/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.7713 - val_loss: 173.6772\n",
      "Epoch 745/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.1285 - val_loss: 174.1939\n",
      "Epoch 746/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.3735 - val_loss: 174.2602\n",
      "Epoch 747/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1885 - val_loss: 174.0913\n",
      "Epoch 748/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6091 - val_loss: 173.3602\n",
      "Epoch 749/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5497 - val_loss: 173.3725\n",
      "Epoch 750/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.7768 - val_loss: 174.1329\n",
      "Epoch 751/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5712 - val_loss: 174.6043\n",
      "Epoch 752/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5077 - val_loss: 174.1986\n",
      "Epoch 753/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.4736 - val_loss: 174.1655\n",
      "Epoch 754/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.4798 - val_loss: 175.4382\n",
      "Epoch 755/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.1322 - val_loss: 175.0074\n",
      "Epoch 756/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.5412 - val_loss: 174.6969\n",
      "Epoch 757/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5114 - val_loss: 174.5694\n",
      "Epoch 758/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 55.1112 - val_loss: 175.5128\n",
      "Epoch 759/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5970 - val_loss: 176.2375\n",
      "Epoch 760/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.7190 - val_loss: 175.5733\n",
      "Epoch 761/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0233 - val_loss: 175.0469\n",
      "Epoch 762/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3358 - val_loss: 174.5732\n",
      "Epoch 763/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.8057 - val_loss: 174.4583\n",
      "Epoch 764/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 53.7995 - val_loss: 174.8638\n",
      "Epoch 765/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 54.7234 - val_loss: 174.3231\n",
      "Epoch 766/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2234 - val_loss: 173.8918\n",
      "Epoch 767/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9271 - val_loss: 173.5663\n",
      "Epoch 768/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9645 - val_loss: 174.1005\n",
      "Epoch 769/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1699 - val_loss: 174.7876\n",
      "Epoch 770/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.8628 - val_loss: 174.9550\n",
      "Epoch 771/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.7238 - val_loss: 174.8790\n",
      "Epoch 772/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.4400 - val_loss: 174.7791\n",
      "Epoch 773/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6981 - val_loss: 175.0378\n",
      "Epoch 774/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1814 - val_loss: 175.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 775/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.1792 - val_loss: 174.7052\n",
      "Epoch 776/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 53.3313 - val_loss: 174.8493\n",
      "Epoch 777/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3890 - val_loss: 175.5319\n",
      "Epoch 778/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 54.5048 - val_loss: 174.7684\n",
      "Epoch 779/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9017 - val_loss: 174.9356\n",
      "Epoch 780/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6569 - val_loss: 175.0238\n",
      "Epoch 781/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2453 - val_loss: 174.6198\n",
      "Epoch 782/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.8300 - val_loss: 174.3403\n",
      "Epoch 783/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0786 - val_loss: 174.5279\n",
      "Epoch 784/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.2998 - val_loss: 174.8067\n",
      "Epoch 785/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0925 - val_loss: 174.5753\n",
      "Epoch 786/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3934 - val_loss: 173.7640\n",
      "Epoch 787/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0422 - val_loss: 173.7726\n",
      "Epoch 788/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 54.0063 - val_loss: 175.1781\n",
      "Epoch 789/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3281 - val_loss: 176.2999\n",
      "Epoch 790/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5238 - val_loss: 175.1042\n",
      "Epoch 791/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0079 - val_loss: 174.4962\n",
      "Epoch 792/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5691 - val_loss: 174.0333\n",
      "Epoch 793/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.7982 - val_loss: 173.9307\n",
      "Epoch 794/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.7631 - val_loss: 174.5195\n",
      "Epoch 795/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3245 - val_loss: 174.9245\n",
      "Epoch 796/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8504 - val_loss: 174.3243\n",
      "Epoch 797/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0652 - val_loss: 173.4453\n",
      "Epoch 798/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 52.7396 - val_loss: 172.8480\n",
      "Epoch 799/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 52.2565 - val_loss: 172.7172\n",
      "Epoch 800/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.5546 - val_loss: 173.4685\n",
      "Epoch 801/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6475 - val_loss: 172.9394\n",
      "Epoch 802/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6863 - val_loss: 172.9898\n",
      "Epoch 803/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.9297 - val_loss: 173.3218\n",
      "Epoch 804/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.6481 - val_loss: 173.7452\n",
      "Epoch 805/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6845 - val_loss: 174.2055\n",
      "Epoch 806/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.4716 - val_loss: 173.4902\n",
      "Epoch 807/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5567 - val_loss: 174.2682\n",
      "Epoch 808/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3069 - val_loss: 174.6382\n",
      "Epoch 809/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.4548 - val_loss: 173.5721\n",
      "Epoch 810/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.9807 - val_loss: 173.2533\n",
      "Epoch 811/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5526 - val_loss: 174.2324\n",
      "Epoch 812/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4319 - val_loss: 173.9964\n",
      "Epoch 813/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.7766 - val_loss: 173.7692\n",
      "Epoch 814/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2290 - val_loss: 173.6380\n",
      "Epoch 815/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4583 - val_loss: 173.2897\n",
      "Epoch 816/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6143 - val_loss: 174.1915\n",
      "Epoch 817/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5672 - val_loss: 174.2626\n",
      "Epoch 818/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.9705 - val_loss: 173.5849\n",
      "Epoch 819/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4498 - val_loss: 173.1743\n",
      "Epoch 820/1000\n",
      "37114/37114 [==============================] - 1s 30us/sample - loss: 52.8298 - val_loss: 172.4002\n",
      "Epoch 821/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 52.8012 - val_loss: 172.2621\n",
      "Epoch 822/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0017 - val_loss: 172.8352\n",
      "Epoch 823/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6901 - val_loss: 173.4522\n",
      "Epoch 824/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6211 - val_loss: 173.3149\n",
      "Epoch 825/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0193 - val_loss: 173.3563\n",
      "Epoch 826/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9420 - val_loss: 173.3790\n",
      "Epoch 827/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2353 - val_loss: 174.0488\n",
      "Epoch 828/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6055 - val_loss: 173.1592\n",
      "Epoch 829/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4225 - val_loss: 173.1126\n",
      "Epoch 830/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7866 - val_loss: 172.4899\n",
      "Epoch 831/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.6121 - val_loss: 173.3750\n",
      "Epoch 832/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6520 - val_loss: 174.3621\n",
      "Epoch 833/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 53.3490 - val_loss: 174.2952\n",
      "Epoch 834/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6480 - val_loss: 174.1287\n",
      "Epoch 835/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6078 - val_loss: 173.9472\n",
      "Epoch 836/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.4782 - val_loss: 173.5545\n",
      "Epoch 837/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0885 - val_loss: 173.4687\n",
      "Epoch 838/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.1551 - val_loss: 173.0707\n",
      "Epoch 839/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8229 - val_loss: 174.2855\n",
      "Epoch 840/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8735 - val_loss: 173.8384\n",
      "Epoch 841/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.2532 - val_loss: 173.3792\n",
      "Epoch 842/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0014 - val_loss: 174.3814\n",
      "Epoch 843/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5364 - val_loss: 173.5361\n",
      "Epoch 844/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0652 - val_loss: 173.4905\n",
      "Epoch 845/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 54.0870 - val_loss: 174.0789\n",
      "Epoch 846/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1249 - val_loss: 173.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1411 - val_loss: 173.9314\n",
      "Epoch 848/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5428 - val_loss: 173.6025\n",
      "Epoch 849/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.6612 - val_loss: 174.4303\n",
      "Epoch 850/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0827 - val_loss: 173.2749\n",
      "Epoch 851/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5245 - val_loss: 172.6824\n",
      "Epoch 852/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8220 - val_loss: 173.3212\n",
      "Epoch 853/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8416 - val_loss: 173.9347\n",
      "Epoch 854/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3298 - val_loss: 173.7660\n",
      "Epoch 855/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.2982 - val_loss: 173.0300\n",
      "Epoch 856/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7014 - val_loss: 173.0798\n",
      "Epoch 857/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9251 - val_loss: 173.1322\n",
      "Epoch 858/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9607 - val_loss: 173.2626\n",
      "Epoch 859/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5250 - val_loss: 172.3187\n",
      "Epoch 860/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2507 - val_loss: 172.4819\n",
      "Epoch 861/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2172 - val_loss: 172.8241\n",
      "Epoch 862/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.3698 - val_loss: 172.4657\n",
      "Epoch 863/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.2742 - val_loss: 173.1750\n",
      "Epoch 864/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2671 - val_loss: 172.6390\n",
      "Epoch 865/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7607 - val_loss: 173.2934\n",
      "Epoch 866/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 52.7890 - val_loss: 172.1892\n",
      "Epoch 867/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 53.6636 - val_loss: 172.1487\n",
      "Epoch 868/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6463 - val_loss: 173.2056\n",
      "Epoch 869/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6149 - val_loss: 172.8406\n",
      "Epoch 870/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2208 - val_loss: 173.5315\n",
      "Epoch 871/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5250 - val_loss: 173.1801\n",
      "Epoch 872/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8112 - val_loss: 172.9466\n",
      "Epoch 873/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0510 - val_loss: 173.0289\n",
      "Epoch 874/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.0615 - val_loss: 173.0527\n",
      "Epoch 875/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9749 - val_loss: 173.1031\n",
      "Epoch 876/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7976 - val_loss: 172.7643\n",
      "Epoch 877/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.9502 - val_loss: 173.0472\n",
      "Epoch 878/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.9219 - val_loss: 173.5352\n",
      "Epoch 879/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7059 - val_loss: 173.8233\n",
      "Epoch 880/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0832 - val_loss: 172.8769\n",
      "Epoch 881/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.6653 - val_loss: 172.2187\n",
      "Epoch 882/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4913 - val_loss: 172.4491\n",
      "Epoch 883/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 53.0324 - val_loss: 171.8823\n",
      "Epoch 884/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9879 - val_loss: 172.6675\n",
      "Epoch 885/1000\n",
      "37114/37114 [==============================] - 1s 31us/sample - loss: 52.0055 - val_loss: 171.7769\n",
      "Epoch 886/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6879 - val_loss: 172.2614\n",
      "Epoch 887/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.1540 - val_loss: 171.9948\n",
      "Epoch 888/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3417 - val_loss: 172.6304\n",
      "Epoch 889/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4456 - val_loss: 172.9945\n",
      "Epoch 890/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5366 - val_loss: 172.3287\n",
      "Epoch 891/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2721 - val_loss: 173.7967\n",
      "Epoch 892/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4489 - val_loss: 173.0840\n",
      "Epoch 893/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.3629 - val_loss: 173.0884\n",
      "Epoch 894/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8815 - val_loss: 173.1252\n",
      "Epoch 895/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3202 - val_loss: 173.3668\n",
      "Epoch 896/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3504 - val_loss: 173.0229\n",
      "Epoch 897/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2398 - val_loss: 173.7328\n",
      "Epoch 898/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4517 - val_loss: 173.4377\n",
      "Epoch 899/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.8220 - val_loss: 173.4012\n",
      "Epoch 900/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3853 - val_loss: 174.2974\n",
      "Epoch 901/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.1451 - val_loss: 173.6302\n",
      "Epoch 902/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2083 - val_loss: 173.8668\n",
      "Epoch 903/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0799 - val_loss: 172.5145\n",
      "Epoch 904/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4858 - val_loss: 172.4831\n",
      "Epoch 905/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.1356 - val_loss: 172.3089\n",
      "Epoch 906/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3385 - val_loss: 172.2537\n",
      "Epoch 907/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9205 - val_loss: 171.8934\n",
      "Epoch 908/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.2884 - val_loss: 172.5488\n",
      "Epoch 909/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2454 - val_loss: 173.0712\n",
      "Epoch 910/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 53.0153 - val_loss: 172.9568\n",
      "Epoch 911/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3378 - val_loss: 172.5615\n",
      "Epoch 912/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0527 - val_loss: 172.9011\n",
      "Epoch 913/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7310 - val_loss: 172.8938\n",
      "Epoch 914/1000\n",
      "37114/37114 [==============================] - 1s 32us/sample - loss: 51.3926 - val_loss: 171.7518\n",
      "Epoch 915/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5569 - val_loss: 172.3972\n",
      "Epoch 916/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.3946 - val_loss: 173.1002\n",
      "Epoch 917/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.7139 - val_loss: 173.3320\n",
      "Epoch 918/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1574 - val_loss: 174.4073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 919/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1372 - val_loss: 173.5991\n",
      "Epoch 920/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6817 - val_loss: 173.9417\n",
      "Epoch 921/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.0367 - val_loss: 173.2182\n",
      "Epoch 922/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5901 - val_loss: 172.8603\n",
      "Epoch 923/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 53.5153 - val_loss: 172.1604\n",
      "Epoch 924/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9967 - val_loss: 172.3978\n",
      "Epoch 925/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1289 - val_loss: 173.3374\n",
      "Epoch 926/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9659 - val_loss: 173.3286\n",
      "Epoch 927/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 52.1090 - val_loss: 174.4517\n",
      "Epoch 928/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0881 - val_loss: 173.9138\n",
      "Epoch 929/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0699 - val_loss: 173.5339\n",
      "Epoch 930/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9910 - val_loss: 173.7075\n",
      "Epoch 931/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9808 - val_loss: 173.1925\n",
      "Epoch 932/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1921 - val_loss: 172.6524\n",
      "Epoch 933/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5853 - val_loss: 172.2737\n",
      "Epoch 934/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.6338 - val_loss: 173.1483\n",
      "Epoch 935/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4988 - val_loss: 173.2426\n",
      "Epoch 936/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4344 - val_loss: 172.9323\n",
      "Epoch 937/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3094 - val_loss: 173.1745\n",
      "Epoch 938/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6341 - val_loss: 173.1214\n",
      "Epoch 939/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5376 - val_loss: 172.8180\n",
      "Epoch 940/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9044 - val_loss: 172.5949\n",
      "Epoch 941/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.6932 - val_loss: 172.4098\n",
      "Epoch 942/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0257 - val_loss: 172.9197\n",
      "Epoch 943/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 51.8547 - val_loss: 172.9432\n",
      "Epoch 944/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6810 - val_loss: 173.2161\n",
      "Epoch 945/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9785 - val_loss: 173.9709\n",
      "Epoch 946/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 51.8181 - val_loss: 173.4019\n",
      "Epoch 947/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0029 - val_loss: 172.8694\n",
      "Epoch 948/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1578 - val_loss: 172.4571\n",
      "Epoch 949/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.3236 - val_loss: 172.7196\n",
      "Epoch 950/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0531 - val_loss: 173.5984\n",
      "Epoch 951/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1466 - val_loss: 172.7808\n",
      "Epoch 952/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.8972 - val_loss: 172.6135\n",
      "Epoch 953/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2520 - val_loss: 172.9669\n",
      "Epoch 954/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0551 - val_loss: 172.8725\n",
      "Epoch 955/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2028 - val_loss: 173.5056\n",
      "Epoch 956/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2401 - val_loss: 173.8457\n",
      "Epoch 957/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5894 - val_loss: 173.3251\n",
      "Epoch 958/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.3165 - val_loss: 172.9793\n",
      "Epoch 959/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6572 - val_loss: 173.5660\n",
      "Epoch 960/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 51.5520 - val_loss: 174.0769\n",
      "Epoch 961/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1374 - val_loss: 173.2886\n",
      "Epoch 962/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.4646 - val_loss: 173.0313\n",
      "Epoch 963/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9785 - val_loss: 173.0920\n",
      "Epoch 964/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8817 - val_loss: 173.1689\n",
      "Epoch 965/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6156 - val_loss: 173.0404\n",
      "Epoch 966/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8858 - val_loss: 171.9015\n",
      "Epoch 967/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1955 - val_loss: 172.2328\n",
      "Epoch 968/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 50.8573 - val_loss: 173.1995\n",
      "Epoch 969/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5582 - val_loss: 172.5535\n",
      "Epoch 970/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.0336 - val_loss: 172.3963\n",
      "Epoch 971/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6402 - val_loss: 173.4095\n",
      "Epoch 972/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.5179 - val_loss: 173.1817\n",
      "Epoch 973/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8388 - val_loss: 173.7171\n",
      "Epoch 974/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.3432 - val_loss: 173.1369\n",
      "Epoch 975/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9572 - val_loss: 172.4018\n",
      "Epoch 976/1000\n",
      "37114/37114 [==============================] - 1s 14us/sample - loss: 51.9305 - val_loss: 171.8949\n",
      "Epoch 977/1000\n",
      "37114/37114 [==============================] - 1s 34us/sample - loss: 51.9651 - val_loss: 171.3138\n",
      "Epoch 978/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8461 - val_loss: 171.6089\n",
      "Epoch 979/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.7546 - val_loss: 172.6769\n",
      "Epoch 980/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.9942 - val_loss: 172.9373\n",
      "Epoch 981/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 50.4979 - val_loss: 173.0300\n",
      "Epoch 982/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.0127 - val_loss: 172.6354\n",
      "Epoch 983/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.2545 - val_loss: 172.6485\n",
      "Epoch 984/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.7891 - val_loss: 173.4141\n",
      "Epoch 985/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8251 - val_loss: 172.5304\n",
      "Epoch 986/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 52.0418 - val_loss: 172.0251\n",
      "Epoch 987/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.0193 - val_loss: 172.3739\n",
      "Epoch 988/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.5843 - val_loss: 171.9310\n",
      "Epoch 989/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4766 - val_loss: 172.5625\n",
      "Epoch 990/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.6773 - val_loss: 172.8812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 991/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.9164 - val_loss: 172.9357\n",
      "Epoch 992/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.4419 - val_loss: 173.0470\n",
      "Epoch 993/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 52.1446 - val_loss: 173.0931\n",
      "Epoch 994/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.0259 - val_loss: 173.2766\n",
      "Epoch 995/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 51.8708 - val_loss: 173.9113\n",
      "Epoch 996/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 50.7391 - val_loss: 173.8306\n",
      "Epoch 997/1000\n",
      "37114/37114 [==============================] - 1s 15us/sample - loss: 50.7689 - val_loss: 173.4866\n",
      "Epoch 998/1000\n",
      "37114/37114 [==============================] - 1s 16us/sample - loss: 51.2904 - val_loss: 174.1196\n",
      "Epoch 999/1000\n",
      "37114/37114 [==============================] - 0s 13us/sample - loss: 51.8148 - val_loss: 173.7303\n",
      "Epoch 1000/1000\n",
      "37114/37114 [==============================] - 1s 13us/sample - loss: 52.2150 - val_loss: 172.5161\n",
      "dict_keys(['loss', 'val_loss'])\n",
      "7.460027003211824\n",
      "0.9975526472457042 0.921238515531058 0.9262979039616281\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.Input(shape=4001,))\n",
    "model.add(keras.layers.Dense(256, activation='relu', kernel_initializer = 'normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "# model.add(keras.layers.Dense(512, activation='relu', kernel_initializer = 'normal'))\n",
    "# model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(units = 1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer= 'adam',loss = 'mean_squared_error')\n",
    "\n",
    "checkpoint_filepath = '/log'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "    \n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                    batch_size=10000, epochs=1000, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "print(history.history.keys())\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "y_train_nn = model.predict(X_train)\n",
    "y_val_nn = model.predict(X_val)\n",
    "y_test_nn= model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "abs_er = mean_absolute_error(y_test, y_test_nn)\n",
    "print(abs_er)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "rsq_train = r2_score(y_train, y_train_nn)\n",
    "rsq_val = r2_score(y_val, y_val_nn)\n",
    "rsq_test = r2_score(y_test, y_test_nn)\n",
    "\n",
    "print(rsq_train, rsq_val, rsq_test)\n",
    "# 100 epoches: abs_er= 19.991914022416207; rsq =0.6311475230416835; rsq_train = 0.9392278263642062 \n",
    "# 20.781554059329675\n",
    "# 0.6444936458044239 0.5632780303898981 0.5536371000303386\n",
    "\n",
    "#3000 epoch (512/256/512) abs=23.203737412144314  r2 = 0.768495009799099  0.5852656900697112 0.5530109538595611\n",
    "#3000 epoch (512/0.3,256/0.3, 512)abs=20.688904906315482 r2 =0.9325278290184188 0.6484640725710789 0.61039924081627\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
