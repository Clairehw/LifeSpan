{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Insight1\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math, re, time, random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "os.chdir(\"I:\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# os.chdir(\"C:\\\\Users\\\\Insight1\\\\Anaconda3\")\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preprocess and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.read_pickle('notes_with_age_and_impairment.pkl')\n",
    "data = pd_read_pickle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['note_split'] = data['notes'].str.replace('\\n', '').str.replace(r' +', ' ').str.split(' ') #replace \\n with space or not\n",
    "data['len_word'] = data['note_split'].apply(len)\n",
    "\n",
    "print(df.shape)\n",
    "df.drop(index = df[(df['len_word'] <=2) | (df['len_word']>= 2000)].index, inplace = True) # remonve empty or >2000 note\n",
    "df.drop(index = df[df['le_months']>250].index, inplace = True) # remove life expect > 250 mo (21 years)\n",
    "# above total removed 706 data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-754d4d795267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spacy_vec_split_len.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import string\n",
    "from scipy import stats\n",
    "def clean2(note):\n",
    "    table = str.maketrans('', '','!\"#$%&\\'()*+,-./:;<=>?@‘¥£’—“[\\\\]^_`{|}~«»§é')\n",
    "    s = note.translate(table)\n",
    "    s1 = s.replace('\\n',' ').lower()\n",
    "    s2 = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', s1)\n",
    "    st_clean = re.sub(r' +', ' ', s2)\n",
    "    return st_clean\n",
    "\n",
    "df['notes_clean'] = df['notes'].progress_apply(lambda x: clean2(x))\n",
    "df['len_clean'] = df['notes_clean'].progress_apply(lambda x: len(x.split(' ')))\n",
    "df[['key','notes','le_months','notes_clean','len_clean']].to_pickle('notes_clean.pkl')\n",
    "\n",
    "df['noteclean_spacy_vec'] = df['notes_clean'].progress_apply(lambda x: nlp(x).vector)\n",
    "df[['notes_clean','noteclean_spacy_vec','le_months']].to_pickle('clean_note_spacy_vec.pkl')\n",
    "\n",
    "filt = (df['len_clean']<=2000) & (df['len_clean']>=100) & (df['le_months'] <=250)\n",
    "df[filt]\n",
    "\n",
    "\n",
    "df['note_spacy_vec'] = df['notes'].progress_apply(lambda x: nlp(x).vector)\n",
    "df.to_pickle('spacy_vec.pkl') #save Spacy vectorized notes to pkl\n",
    "\n",
    "\n",
    "df['le_months'] = df['le_months'].astype(int)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df['le_months'].astype(int).min())\n",
    "print(df['le_months'].astype(int).max())\n",
    "\n",
    "ave_mo = np.average(df['le_months'].to_numpy())\n",
    "mode_mo = stats.mode(df['le_months'].to_numpy())\n",
    "print('Months to live Average: {} \\nMonths to live mode: {}'.format(ave_mo, mode_mo[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('spacy_vec_split_len.pkl')\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "filt = (df['len_word'] >= 500) & (df['len_word'] <= 600)\n",
    "notewc = df.loc[filt, 'notes'].copy()\n",
    "notewc.index = range(len(notewc))\n",
    "\n",
    "\n",
    "filtwc = (df['len_lss'] >= 500) & (df['len_lss'] <= 600)\n",
    "notewc = df.loc[filtwc, 'lss_note'].copy()\n",
    "notewc.index = range(notewc.shape[0])\n",
    "\n",
    "a = random.randrange(len(notewc))\n",
    "\n",
    "STOPWORDS.update(['noted', 'was', 'say','Was','cited','Vv'])\n",
    "# STOPWORDS.update(['noted', 'was', 'say','Was','cited','Vv','wa'])\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "wc = WordCloud(width = 800, height = 800, \n",
    "               background_color ='white', \n",
    "               stopwords = stopwords)\n",
    "wc.generate(notewc[a])\n",
    "\n",
    "# note = ' '.join(notewc[a][1]) \n",
    "# wc.generate(note)\n",
    "\n",
    "plt.figure(figsize = (6, 6), facecolor = None) \n",
    "plt.imshow(wc) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() \n",
    "wc.to_file('note_wc7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_stem_stop(note):\n",
    "    word_tokens = word_tokenize(note)\n",
    "    \n",
    "    # lemmatization\n",
    "    lemma_words = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for w in word_tokens:\n",
    "        word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "        word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "        word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "        lemma_words.append(word3)\n",
    "\n",
    "    # STOPWORDS\n",
    "    states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "              \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \n",
    "              \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n",
    "              \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \n",
    "              \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "    states = [x.lower() for x in states]\n",
    "    \n",
    "    stopwd = set(stopwords.words('english'))\n",
    "    stopwd.update(['note', 'was', 'say','cite','vv','ii','iii','report', 'disclaim','disclaimer',\n",
    "                   'request','underwrite','underwriter','life','expect','certify','take','company',\n",
    "                  'id','written','write','ssn','social','security','number','llc']) \n",
    "    # add 'use'\n",
    "    stopwd.update(states)\n",
    "    filtered_sentence = [w for w in lemma_words if not w in stopwd] # can switch lemma or stem words\n",
    "    \n",
    "    #Stemming\n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    \n",
    "    lemma_stop_stem = Stem_words\n",
    "    return lemma_stop_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "df['spacy_lss'] = df['lss_note'].progress_apply(lambda x: nlp(' '.join(x[1])).vector)\n",
    "\n",
    "df[['spacy_lss','len_lss','le_months']].to_pickle('spacy_lss.pkl')\n",
    "\n",
    "df = pd.read_pickle('spacy_vec_split_len.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'requesting company coventry  valley green rd fort washington pa   med rec from  to  kenneth wayne dulyea has history of diabetes with peripheral circulatory disorder hyperlipidemia cad htn ckd osa  tia  obesity and nqncqmpiaijpeysjrgeries included pci with stent of rca i   diabetes was documented as diet controlled glucose was  and alc   aflcwas  actos was prescribed  october bp was   afcin march was  actos was increased july bp was   january bp was   prostate biopsy in january was benign bp was  hctz was prescribed march glucbsewas  and alc  glucophage was increased  february glucose was  and ac   shortness of breath with vertigo was documented in february bp was  ekg showed normal sinus rhythm  with nonspecific inferior twave abnormalities unstable angina was reported in march pci with stent of rcawas done b secondary to total occlusion ekg showed sinus rhythm with some stt changes of nonspecific nature pyuria and hematuria were identified vp showed calcified mass on the floor of the bladder cystoscopy revealed an enlarged prostate uroxatral was prescribed may glucose was  and a¢  myoview in june was normal ef was  august bp was  glucose was  and ac   may glucose was   july bp was  alcwas  in october diabetes was poorly controlled  january egfr was  br in may was  diabetes was described as worse and medication was not taken as prescribed the use of humalog lantus and prandin was noted diabetes with peripheral circulatory disorder was recorded in august   shortness of breath was noted in february echo in march revealed no compromise and ef  bilateral lower  extremity edema was present moderately severe dyspnea was reportedsyrﬁbtoms were aggravated by mild activity such as walking chest pain was denied medication for diabetes was not taken regularly glucose was  egfr  and ac   weights were not recorded and diuretic was not increased as advised in may noncompliance was cited in august december ac was  and egfr  tf   march bp was  ac was  compliance with diet was fair in april complaints of shortness of breath and sleep apnea were documented in june chest pressure with an inability to get air in was noted substernal chest pain was denied carotid us showed  bilateral stenosis myoview was negative ef was  shortness of breath improved after taking prescribed lasix ckd was listed in august osa was treated with cpap an  april ac was  and egfr  fall with facial abrasions occurred in october ct facial bones was negative for fracture upon assessment no assistive device was needed in november admission occurred for acute tia complaints of upper extremity weakness confusion and blurred vision  episodes were recorded morbid obesity was listed ct found no acute intragranial process glucose was  and egfr  bp in december was  insulin was not taken regularly  may urine showed glucose  glucose was  egfr  and ac  cpap continued for osa in june gcytoscopy was planned for microhematuria urine cytology was negative for malignancy lifestyle was described as  inactive  please see disclaimer on final page disclaimer has full force and effect as if written in full here au untriwiting ossn cevsintriwrdingy o e   townpark drive suite  kennesaw georgia    underwriting ge di page  of  name dulyea kenneth wayne st ssn  report version   weight loss was mentioned in may risk of falls was recorded he remained varyqqnoneompﬂant victoza was started ac was  september glucose was  egfr  ac  and urine microalbumin  history of tia  was documented in november symptoms had completely resolved nausea and vomiting were reported on victoza in november lantus was prescribed prior to each meal but was only taken  times per day chest pain and dyspnea were denied bp was  medications included cozaar uroxatral lopressor lantus bumex humalog and potassium dateof uw  this report supersedes all previous reports  repgt processed afjd completed by avs underwriting llc el o bkl owanne odonnell underwriter reported le is siatistical mean disclaimer '"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import digits\n",
    "\n",
    "def remove_number(note):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    no_num = note.translate(remove_digits)\n",
    "    return no_num\n",
    "\n",
    "remove_number(df['notes_clean'][0]) #should i remove numbers? eg A1c would be ac? try later\n",
    "\n",
    "\n",
    "#df['len_char'] = df['notes'].apply(len)\n",
    "df['note_split'] = df['notes'].str.replace('\\n', '').str.replace(r' +', ' ').str.split(' ') #replace \\n with space or not\n",
    "df['len_word'] = df['note_split'].apply(len)\n",
    "\n",
    "df.to_pickle('spacy_vec_split_len.pkl') #save Spacy vectorized notes to pkl\n",
    "\n",
    "\n",
    "# df = pd.read_pickle('spacy_vec_split_len.pkl')\n",
    "# type(df['note_split'][0])\n",
    "\n",
    "\n",
    "df['note_spacy_vec'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y from noteclean_spacy_vec\n",
    "df = pd.read_pickle('clean_note_spacy_vec.pkl')\n",
    "df2 = pd.read_pickle('notes_clean.pkl')\n",
    "df['len_clean'] = df2['len_clean']\n",
    "\n",
    "filt = df[(df['len_clean']>=100) & (df['len_clean']<=2000) & (df['le_months']<= 250)] # filter data for ML\n",
    "\n",
    "X = np.array(filt['noteclean_spacy_vec'].values.tolist())\n",
    "y = filt['le_months'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnn = data[(data['le_months'] <= 250) & (data['len_word'] <= 2000)]\n",
    "X = np.array(dfnn['note_spacy_vec'].tolist())\n",
    "# y = dfnn['le_months'].astype(np.float32).to_numpy()\n",
    "y = dfnn['le_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('notes_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddb7d96dedb40b5a468ca156b615c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['lss_note'] = df['notes_clean'].progress_apply(lambda x: lemma_stem_stop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380132ffd33048d29c7a35b6e32df708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['len_lss'] = df['lss_note'].progress_apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f358b52682404bb7fde7601f830191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['lss_corpus'] = df['lss_note'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"I:\")\n",
    "df.to_pickle('lemma_stem_stop_filtered_note.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (df['len_lss'] <=1000) & (df['len_lss'] >= 50) & (df['le_months']<=250 )\n",
    "data = df[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of Words per Note')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['len_lss'].to_numpy(), bins = 50, color = 'pink', ec = 'black')\n",
    "plt.xlabel('Words Per Note',fontsize =12)\n",
    "plt.ylabel('Frequency',fontsize =12)\n",
    "plt.xlim(-50,1050)\n",
    "plt.title('Number of Words per Note')#### Plot life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y from noteclean_spacy_vec\n",
    "filt = df[(df['len_clean']>=100) & (df['len_clean']<=2000) & (df['le_months']<= 250)] # filter data for ML\n",
    "\n",
    "X = np.array(filt['noteclean_spacy_vec'].values.tolist())\n",
    "y = filt['le_months'].to_numpy()\n",
    "\n",
    "\n",
    "# X and y from note_spacy_vec\n",
    "filt = df[(df['len_word']>3) & (df['len_word']<=2000) & (df['le_months']<= 250)] # filter data for ML\n",
    "# filt['len_word'].value_counts().sort_index()\n",
    "\n",
    "X = np.array(filt['note_spacy_vec'].values.tolist())\n",
    "y = filt['le_months'].to_numpy()\n",
    "\n",
    "\n",
    "# X and y from spacy_lss\n",
    "df = pd.read_pickle('spacy_lss.pkl')\n",
    "filt = df[(df['len_lss']>50) & (df['len_lss']<=1000) & (df['le_months']<= 250)] # filter data for ML\n",
    "# filt['len_word'].value_counts().sort_index()\n",
    "\n",
    "X = np.array(filt['spacy_lss'].values.tolist())\n",
    "y = filt['le_months'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# transform len_lss\n",
    "filt = (df['len_lss']>50) & (df['len_lss']<=1000) & (df['le_months']<= 250)  # filter data for ML\n",
    "data = df[filt][['le_months','lss_note','lss_corpus']]\n",
    "\n",
    "corpus = data['lss_corpus'].tolist()\n",
    "vec = CountVectorizer()\n",
    "bag_of_words = vec.fit_transform(corpus)\n",
    "sum_words = bag_of_words.sum(axis=0) # sum word counts\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# return words_freq[:n]\n",
    "vocab = [i[0] for i in words_freq[:4000]] # define vocabulary for tf-idf 4000 words\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "corpus_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X = corpus_tfidf.toarray()\n",
    "y = data['le_months']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "X = lda.fit_tranform(data)\n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg_lr = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_lr = reg_lr.predict(X_test)\n",
    "print(reg_lr.score(X_test, y_test)) # R^2 variance weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb regressor r2 score = 0.4118154583805178 for 57k data\n",
    "#xgb reg spacy_lss r2= 0.44425965934761835  for 50-1000 len_lss, le_months<250\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor().fit(X_train,y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(r2_score(y_test, y_pred_xgb, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge regressor score = 0.4366820387501755, basically the model didn't work. \n",
    "# std for predicted label is nearly 60% of predicted value.\n",
    "from sklearn import linear_model\n",
    "reg_br = linear_model.BayesianRidge()\n",
    "reg_br.fit(X_train, y_train)\n",
    "\n",
    "print(reg_br.score(X_test, y_test))\n",
    "print(reg_br.predict(X_test[:10], return_std = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor score = 0.5402527642512915 for 57k data\n",
    "# rf regressor r2 = 0.5560166555170266 when filted len_word<=750, le_months<=250\n",
    "# rf regressor r2 = 0.5384426783146021 when filted 100<=len_word<=2000, le_months<=250 on noteclean_spacy_vec\n",
    "#rf reg spacy_lss r2= 0.5800088178042535 for 50-1000 len_lss, le_months<250\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "reg_rf = RandomForestRegressor(n_estimators = 100, random_state = 0) \n",
    "reg_rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "y_test_rf = reg_rf.predict(X_test)\n",
    "y_train_rf = reg_rf.predict(X_train)\n",
    "\n",
    "print(reg_rf.score(X_train, y_train), reg_rf.score(X_test, y_test))\n",
    "# r2 = 0.9483540267963761 0.6502525420270096 for n_estimator 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor score = 0.5402527642512915 for 57k data\n",
    "# rf regressor r2 = 0.5560166555170266 when filted len_word<=750, le_months<=250\n",
    "# rf regressor r2 = 0.5384426783146021 when filted 100<=len_word<=2000, le_months<=250 on noteclean_spacy_vec\n",
    "#rf reg spacy_lss r2= 0.5800088178042535 for 50-1000 len_lss, le_months<250\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "reg_rf = RandomForestRegressor(n_estimators = 100, random_state = 0) \n",
    "reg_rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "y_test_rf = reg_rf.predict(X_test)\n",
    "y_train_rf = reg_rf.predict(X_train)\n",
    "\n",
    "print(reg_rf.score(X_train, y_train), reg_rf.score(X_test, y_test))\n",
    "# r2 = 0.9483540267963761 0.6502525420270096 for n_estimator 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "r1 = LinearRegression()\n",
    "r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
    "X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
    "y = np.array([2, 6, 12, 20, 30, 42])\n",
    "er = VotingRegressor([('lr', r1), ('rf', r2)])\n",
    "print(er.fit(X, y).predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input(shape=(300,)))\n",
    "model.add(keras.layers.Dense(256, activation='relu', kernel_initializer = 'normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "# model.add(keras.layers.Dense(512, activation='relu', kernel_initializer = 'normal'))\n",
    "# model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(units = 1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer= 'adam',loss = 'mean_squared_error')\n",
    "\n",
    "checkpoint_filepath = '/log'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "    \n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                    batch_size=10000, epochs=3000, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "print(model_log.history.keys())\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred= model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "abs_er = mean_absolute_error(y_test, y_test_pred)\n",
    "print(abs_er)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "rsq_train = r2_score(y_train, y_train_pred)\n",
    "rsq_val = r2_score(y_val, y_val_pred)\n",
    "rsq_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(rsq_train, rsq_val, rsq_test)\n",
    "# 100 epoches: abs_er= 19.991914022416207; rsq =0.6311475230416835; rsq_train = 0.9392278263642062 \n",
    "# 20.781554059329675\n",
    "# 0.6444936458044239 0.5632780303898981 0.5536371000303386\n",
    "\n",
    "#3000 epoch (512/256/512) abs=23.203737412144314  r2 = 0.768495009799099  0.5852656900697112 0.5530109538595611\n",
    "#3000 epoch (512/0.3,256/0.3, 512)abs=20.688904906315482 r2 =0.9325278290184188 0.6484640725710789 0.6103992408162731\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfnn['le_halfyear'] = dfnn['le_months'].apply(lambda x: round(x/6))\n",
    "\n",
    "dfnn['le_halfyear'].value_counts().sort_index()\n",
    "\n",
    "X = np.array(dfnn['note_spacy_vec'].tolist())\n",
    "label = dfnn['le_halfyear']\n",
    "\n",
    "X_trainval, X_test, label_trainval, label_test = train_test_split(X, label, test_size=0.2, random_state=42)\n",
    "X_train, X_val, label_train, label_val = train_test_split(X_trainval, label_trainval, test_size=0.125, random_state=42)\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "\n",
    "label_train_pred = np.argmax(model.predict(X_train),1)\n",
    "label_val_pred = np.argmax(model.predict(X_val),1)\n",
    "label_test_pred= np.argmax(model.predict(X_test),1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "rsq_train = accuracy_score(label_train, label_train_pred)\n",
    "rsq_val = accuracy_score(label_val, label_val_pred)\n",
    "rsq_test = accuracy_score(label_test, label_test_pred)\n",
    "\n",
    "print(rsq_train, rsq_val, rsq_test)\n",
    "# 0.7535449361659181 0.2555634051571883 0.25633554083885207\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hist(data, bins=50, color='gold', ec='black', xlabel='Month', ylabel='Frequency', title='title')\n",
    "    plt.hist(data, bins=bins, color=color, ec=ec)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel,fontsize=12)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(x, y, figsize=(8,8), label='Test', xlabel='True', y='Predicted', title='Random Forest Regressor')\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x, y, label = label)\n",
    "    plt.plot(range(300), range(300), '-k')\n",
    "    plt.xlim(0, 300)\n",
    "    plt.ylim(0, 300)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel,fontsize =12)\n",
    "    plt.ylabel(ylabel,fontsize =12)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(x, y):\n",
    "    plt.plot(x)\n",
    "    plt.plot(y)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(300, 300)\n",
    "        self.linear_layer = nn.Linear(300, 1)\n",
    "    def forward(self, input_vec):\n",
    "        output, _ = self.lstm_layer(input_vec)\n",
    "        logit = self.linear_layer(output.view(len(input_vec), -1))\n",
    "        return logit\n",
    "model = lstm()\n",
    "print(model)\n",
    "\n",
    "for p in model.named_parameters():\n",
    "    print(p[0], str(tuple(p[1].size())))\n",
    "    \n",
    "model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_idmtx(note):\n",
    "    ids = tokenizer.encode(note, add_special_tokens = False)\n",
    "    idmtx = [[101]+ids[i:i+510]+[102] for i in range(0, len(ids), 510)]\n",
    "    idmtx[-1] += (512-len(idmtx[-1]))*[0]\n",
    "    idmtx = np.array(idmtx).reshape(-1, 512)\n",
    "    return idmtx\n",
    "\n",
    "df['len_token_id'] = df['token_id'].progress_apply(len)\n",
    "df['token_id_matrix'] = df['notes'].progress_apply(text_to_idmtx)\n",
    "# df[['key','notes', 'le_months', 'note_spacy_vec', 'bert_token_id','token_id_matrix']].to_csv('note_vec_token.csv', index = False)\n",
    "\n",
    "df[['key','notes', 'le_months', 'note_spacy_vec','token_id_matrix']].to_pickle('note_vec_token.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_row(mtx, k=6):\n",
    "    row = mtx.shape[0]\n",
    "    if row <= k:\n",
    "        n = 512*(k-row)\n",
    "        mtx_new = np.append(mtx.reshape(1, -1), [0]*n)\n",
    "    else:\n",
    "        mtx_new = np.squeeze(mtx.reshape(1,-1))[: k*512]\n",
    "    return mtx_new\n",
    "\n",
    "\n",
    "\n",
    "df['six_row'] = df['token_id_matrix'].progress_apply(k_row)\n",
    "df['six_row'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbert = df[df['le_months'] <= 250]\n",
    "X = np.array(dfbert['six_row'].tolist()).astype(np.int64)\n",
    "y = dfbert['le_months'].astype(np.float32).to_numpy()\n",
    "\n",
    "mask = (X!=0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoader(input_ids, input_mask, labels, batch_size=32, val_test_size=0.1, random_seed=2020):\n",
    "    \"\"\"This function takes in all available data, split into train, val, test, and return a torch DataLoader for each. \n",
    "    \"\"\"\n",
    "    # split into train, validation, test \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(input_ids, labels, random_state=random_seed, test_size=val_test_size)\n",
    "    mask_train_val, mask_test, _, _ = train_test_split(input_mask, labels, random_state=random_seed, test_size=val_test_size)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=random_seed, test_size=val_test_size)\n",
    "    mask_train, mask_val, _, _ = train_test_split(mask_train_val, y_train_val, random_state=random_seed, test_size=val_test_size)\n",
    "\n",
    "    # turn numpy array into tensor\n",
    "    X_train, X_val, X_test  = torch.tensor(X_train), torch.tensor(X_val), torch.tensor(X_test)\n",
    "    y_train, y_val, y_test = torch.tensor(y_train), torch.tensor(y_val), torch.tensor(y_test)\n",
    "    mask_train, mask_val, mask_test = torch.tensor(mask_train), torch.tensor(mask_val), torch.tensor(mask_test)\n",
    "\n",
    "    # Create the DataLoader for training set.\n",
    "    train_data = TensorDataset(X_train, mask_train, y_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for validation set.\n",
    "    validation_data = TensorDataset(X_val, mask_val, y_val)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for testing set.\n",
    "    test_data = TensorDataset(X_test, mask_test, y_test)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    return(train_dataloader, validation_dataloader, test_dataloader)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=1, freeze_bert = True):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        #Instantiating clinical_bert model  \n",
    "        self.clinical_bert_layer = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.clinical_bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(input_size=768, hidden_size=768, num_layers=1, batch_first=False)\n",
    "        self.cls_layer = nn.Linear(768, num_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        batch_size, lstm_length = input_ids.shape[0], int(input_ids.shape[1] / 512)\n",
    "        input_ids, attention_mask = input_ids.view(-1, 512), attention_mask.view(-1, 512)\n",
    "        last_hidden_state , _ = self.clinical_bert_layer(input_ids, attention_mask = attention_mask)\n",
    "        cls_embedding = last_hidden_state[:, 0, :].unsqueeze(1)\n",
    "        cls_embedding = cls_embedding.view(lstm_length, batch_size, -1)\n",
    "        _, (hn, _) = self.lstm_layer(cls_embedding) \n",
    "        cls_holistic = hn[-1]\n",
    "        logits = self.cls_layer(cls_holistic).squeeze()\n",
    "        del input_ids, attention_mask, last_hidden_state, cls_embedding, hn, cls_holistic\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dataloader, val_dataloader, config, scheduler=None):\n",
    "\n",
    "    random.seed(config.random_seed)\n",
    "    np.random.seed(config.random_seed)\n",
    "    torch.manual_seed(config.random_seed)\n",
    "    torch.cuda.manual_seed_all(config.random_seed)\n",
    "\n",
    "      # ========================================\n",
    "      #               Training\n",
    "      # ========================================\n",
    "    iterations = 0\n",
    "    for epoch in tqdm(range(0, config.epochs)):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, config.epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader),desc='Training one epoch'):\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(config.device)\n",
    "        b_input_mask = batch[1].to(config.device)\n",
    "        b_targets = batch[2].to(config.device)\n",
    "\n",
    "        model.zero_grad()  # set gradient to 0 before training\n",
    "        logits = model(b_input_ids, attention_mask=b_input_mask)  # forward pass\n",
    "\n",
    "        loss = criterion(logits, b_targets) # calculate the batch loss\n",
    "        total_loss += loss.item() # accumulate loss in an epoch to compute average loss \n",
    "\n",
    "        elapsed = format_time(time.time() - t0) # Calculate elapsed time in minutes.\n",
    "        if step % config.print_interval == 0 and not step == 0:\n",
    "        print('  Batch {:>5,}  of  {:>5,}.  Elapsed: {:}.  Loss: {:>6.4f}.'.format(step, len(train_dataloader), elapsed, loss))\n",
    "\n",
    "        loss.backward()  # back prop\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip gradient to prevent exploding\n",
    "        optimizer.step()  # update weights\n",
    "        if scheduler:\n",
    "        scheduler.step()  # Update the learning rate.\n",
    "        iterations += 1\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "\n",
    "        if iterations % config.val_interal == 0 and not iterations == 0:\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t1 = time.time()\n",
    "        model.eval()   # Put the model in evaluation mode--the dropout layers behave differently during eval\n",
    "\n",
    "        eval_loss, nb_eval_steps = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in tqdm(val_dataloader, desc='Validation'):\n",
    "\n",
    "            batch = tuple(t.to(config.device) for t in batch)       # Add batch to GPU\n",
    "            b_input_ids, b_input_mask, b_targets = batch      # Unpack the inputs from our dataloader\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            loss = criterion(logits, b_targets) # calculate the batch loss\n",
    "            eval_loss += loss\n",
    "\n",
    "            nb_eval_steps += 1      # Track the number of batches\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  MSE: {0:.4f}\".format(eval_loss/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t1)))\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'iterations': iterations,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss_MAE': eval_loss/nb_eval_steps,\n",
    "            }, config.PATH + '_' + str(iterations))\n",
    "        model.train()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)  # Calculate the average loss over the training data.      \n",
    "\n",
    "    print(\"  Average training MSE: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    batch_size=32\n",
    "    epochs = 10\n",
    "    val_interal=100   # do validation after this number of batches have been trained\n",
    "    print_interval=40 # print the details of the training every this number of batch\n",
    "    val_test_size=0.1  \n",
    "    random_seed=42\n",
    "    PATH = './checkpoints/checkpoint'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "model = Model(num_class=1, freeze_bert=True)\n",
    "model.to(config.device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8 )\n",
    "\n",
    "\n",
    "# # dummy data\n",
    "# input_ids = np.random.randint(1, 10000, (1000, 512))\n",
    "# input_mask = np.ones_like(input_ids)\n",
    "# targets = np.random.randint(1, 5, (1000,))\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = dataLoader(X, mask, y)\n",
    "total_steps = len(train_dataloader) * config.epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 8\n",
    "config.val_interal = 500\n",
    "config.epochs = 10\n",
    "\n",
    "# model = Model(num_class=1, freeze_bert=True)\n",
    "# model.to(config.device)\n",
    "train(model, criterion, optimizer, train_dataloader, val_dataloader, config, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.max_memory_allocated(device=config.device) / 1024 / 1024 / 1024)\n",
    "torch.cuda.memory_allocated(device=config.device) / 1024 / 1024 / 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
